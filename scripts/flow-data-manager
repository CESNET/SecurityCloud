#!/usr/bin/env python3

# author: Jan Wrona, wrona@cesnet.cz

# Copyright (C) 2017 CESNET
#
# LICENSE TERMS
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in
#    the documentation and/or other materials provided with the
#    distribution.
# 3. Neither the name of the Company nor the names of its contributors
#    may be used to endorse or promote products derived from this
#    software without specific prior written permission.
#
# ALTERNATIVELY, provided that this notice is retained in full, this
# product may be distributed under the terms of the GNU General Public
# License (GPL) version 2 or later, in which case the provisions
# of the GPL apply INSTEAD OF those given above.
#
# This software is provided ``as is'', and any express or implied
# warranties, including, but not limited to, the implied warranties of
# merchantability and fitness for a particular purpose are disclaimed.
# In no event shall the company or contributors be liable for any
# direct, indirect, incidental, special, exemplary, or consequential
# damages (including, but not limited to, procurement of substitute
# goods or services; loss of use, data, or profits; or business
# interruption) however caused and on any theory of liability, whether
# in contract, strict liability, or tort (including negligence or
# otherwise) arising in any way out of the use of this software, even
# if advised of the possibility of such damage.


import os
import sys
import re
import math
import argparse
import datetime
import warnings
import atexit
import copy
import shutil
from pathlib import Path


OUTPUT_PREFIX = os.path.basename(__file__).upper() if '__file__' in globals() \
    else 'DATA-MANAGER'

# strftime()/strptime() translation table
STRTIME_DICT = {'year': '%Y', 'month': '%m', 'day': '%d', 'hour': '%H',
                'minute': '%M', 'second': '%S'}
# strftime()/strptime() formats for YYYYMMDDHHMM and YYYYMMDDHHMMSS
STRTIME_FORMAT_YMDHM = (STRTIME_DICT['year'] + STRTIME_DICT['month']
                        + STRTIME_DICT['day'] + STRTIME_DICT['hour']
                        + STRTIME_DICT['minute'])
STRTIME_FORMAT_YMDHMS = STRTIME_FORMAT_YMDHM + STRTIME_DICT['second']
# re match pattern for YYYYMMDDHHMM or YYYYMMDDHHMMSS
DATETIME_PATTERN = (r'(?P<year>\d{4})(?P<month>\d{2})(?P<day>\d{2})'
                    r'(?P<hour>\d{2})(?P<minute>\d{2})'
                    r'(?P<second>\d{2})?')
# precompiled regex for YYYYMMDDHHMM or YYYYMMDDHHMMSS
DATETIME_REGEX = re.compile(DATETIME_PATTERN, re.ASCII)
FLOW_FILE_PREFIX = 'lnf.'
BFINDEX_FILE_PREFIX = 'bfi.'

# metric prefixes for int_prefix()
METRIC_PREFIXES = 'kMGTPEZYyzafpnÂµm'

# file name of the lock file
LOCK_FILE_NAME = 'flow-data-manager.lock'


###############################################################################
class Node:
    """Representation of a node with multiple base profiles.

    In the usual case, list of base profiles contains just a single base
    profile called "live".
    """

    def __init__(self, base_path):
        self.base_path = base_path
        self.profiles = []

    def __repr__(self):
        return '{}(name={}, profiles={})'.format(type(self).__name__,
                                                 self.base_path.name,
                                                 self.profiles)

    def scan(self):
        """Scan node's base directory and look for base profiles.

        Scan node's base directory, look for profiles and eventually
        recursively construct those profiles as instances of class Profile.
        """

        self.profiles.clear()

        vprint('node directory "{}" scan:'.format(self.base_path.name),
               end=' ')
        content_dict = dir_scan(self.base_path)
        vprint('using dirs: {}, ignoring: {}'
               .format(content_dict['dirs_n'], content_dict['others_n']),
               use_prefix=False)
        for profile_dir in content_dict['dirs']:
            profile = Profile(profile_dir, self)
            profile.scan()
            self.profiles.append(profile)


class Profile:
    """IPFIXcol profile/subprofile representation."""

    def __init__(self, base_path, parent_obj):
        self.base_path = base_path  # pathlib path to the profile's base dir
        self.parent_obj = parent_obj  # profile's parent Profile or Node
        self.channels = []  # list of instances of Channel
        self.subprofiles = []  # list of instances of Profile

    def __repr__(self):
        return '{}(name={}, parent_obj={}, channels={}, subprofiles={})'\
            .format(type(self).__name__, self.base_path.name,
                    self.parent_obj.base_path.name,
                    self.channels, self.subprofiles)

    def scan(self):
        """Scan profile's base directory and look for channels and subprofiles.

        Scan profile's base directory and look for for channels (directory
        "channels"), ignore directory with RRD files "rrd", also ignore hidden
        directories. All other directories are assumed to be subprofiles and
        are recursively constructed as instances of class Profile.
        """

        self.channels.clear()
        self.subprofiles.clear()

        vprint('profile directory "{}" scan:'.format(self.base_path.name),
               end=' ')
        content_dict = dir_scan(self.base_path)
        # further separate found directories into channels directory, rrd
        # directory, and other directories == subprofile directories
        for subdir in content_dict['dirs']:
            if subdir.name == 'channels' or subdir.name == 'rrd':
                content_dict[subdir.name + '_dir'] = subdir
                content_dict[subdir.name + '_dir_n'] = subdir.name
            else:
                content_dict.setdefault('subprofile_dirs', set()).add(subdir)
                content_dict.setdefault('subprofile_dirs_n', set()).add(
                    subdir.name)
        content_dict.setdefault('subprofile_dirs', set())
        content_dict.setdefault('subprofile_dirs_n')
        content_dict.setdefault('rrd_dir')
        content_dict.setdefault('rrd_dir_n')

        if 'channels_dir' not in content_dict:
            raise Exception('profile "{}" is missing the "channels" directory'
                            .format(self.base_path.name))
        vprint('channels dir: {}, RRD dir: {}, subprofiles list: {}, '
               'ignoring files: {}'.format(content_dict['channels_dir_n'],
                                           content_dict['rrd_dir_n'],
                                           content_dict['subprofile_dirs_n'],
                                           content_dict['others_n']),
               use_prefix=False)

        # scan channels directory
        vprint('channels directory "{}" scan:'
               .format(content_dict['channels_dir'].name), end=' ')
        ch_content_dict = dir_scan(content_dict['channels_dir'])
        vprint('using dirs: {}, ignoring files: {}'
               .format(ch_content_dict['dirs_n'], ch_content_dict['others_n']),
               use_prefix=False)
        for channel_dir in ch_content_dict['dirs']:
            channel = self.Channel(channel_dir, self)
            channel.scan()
            self.channels.append(channel)

        for subprofile_dir in content_dict['subprofile_dirs']:
            subprofile = Profile(subprofile_dir, self)
            subprofile.scan()
            self.subprofiles.append(subprofile)

    def get_channels(self, recursively=False):
        """Return a list of subprofiles, recursively or not."""

        channels = list(self.channels)  # make a copy
        if recursively:
            for subprofile in self.subprofiles:
                channels += subprofile.get_channels(recursively)
        return channels

    class Channel:
        """IPFIXcol channel representation."""

        def __init__(self, base_path, parent_profile):
            self.base_path = base_path  # pathlib path to the channel's basedir
            self.parent_profile = parent_profile  # channel's parent Profile
            self.flow_files = []  # list of instances of FlowFile
            self.date_dirs = set()  # set of directories

        def __repr__(self):
            return ('{}(name={}, parent_profile={}, earliest_flow_file={}, '
                    'latest_flow_file={})'
                    .format(type(self).__name__, self.base_path.name,
                            self.parent_profile.base_path.name,
                            self.get_earliest_flow_file(),
                            self.get_latest_flow_file()))

        def is_empty(self):
            return False if self.flow_files else True

        def get_earliest_flow_file(self):
            return self.flow_files[0] if not self.is_empty() else None

        def get_latest_flow_file(self):
            return self.flow_files[-1] if not self.is_empty() else None

        def remove_unused_directories(self):
            """ Remove directories from which all flow files were unlinked. """
            contains_ffs = set()  # dirs which contains at least 1 flow file
            may_contain_ffs = set()  # dirst which may conatin some ff
            for flow_file in self.flow_files:
                if flow_file.path.exists():  # ffs which were not unlinked
                    contains_ffs.add(flow_file.path.parent)
                else:  # ffs which were unlinked
                    may_contain_ffs.add(flow_file.path.parent)
            doesnt_contain_ffs = may_contain_ffs - contains_ffs
            doesnt_contain_ffs.discard(self.base_path)  # never remove base dir
            for directory in doesnt_contain_ffs:
                vprint('channel "{}" cleanup: removing directory with no flow '
                       'files "{}"'.format(self.base_path.name, directory))
                self.date_dirs.remove(directory)
                dir_remove(directory, True)
            return len(doesnt_contain_ffs)

        def remove_empty_directories(self):
            """ Repeatedly remove empty directories until no date_dir is empty.
            """
            possibly_empty = copy.deepcopy(self.date_dirs)
            removed_counter = 0
            while possibly_empty:
                possibly_empty.discard(self.base_path)  # never remove base dir
                possibly_empty_next = set()
                for directory in possibly_empty:
                    if dir_is_empty(directory):
                        possibly_empty_next.add(directory.parent)
                        self.date_dirs.remove(directory)
                        vprint('channel "{}" cleanup: removing empty '
                               'directory "{}"'.format(self.base_path.name,
                                                       directory))
                        dir_remove(directory)
                        removed_counter += 1
                possibly_empty = self.date_dirs.intersection(
                        possibly_empty_next)
            return removed_counter

        def scan(self):
            """Scan channel's base directory and look for flow files."""

            self.flow_files.clear()

            vprint('channel directory "{}" scan:'.format(self.base_path.name),
                   end=' ')
            # recursively iterate over all files (excluding base_path)
            for path in self.base_path.rglob('*'):
                if path.is_dir():
                    self.date_dirs.add(path)
                    continue
                elif not path.is_file():
                    warnings.warn('channel scan: found unknown file "{}", '
                                  'which is not regular file nor directory '
                                  'in "{}" channel storage hierarchy, skipping'
                                  .format(path, self.base_path.name))
                    continue
                elif path.name.startswith(FLOW_FILE_PREFIX):
                    flow_file = self.FlowFile(path, self)
                    try:
                        flow_file.parse_path()
                    except ValueError as e:
                        warnings.warn(e.args[0] + ', skipping')
                        continue
                    self.flow_files.append(flow_file)
                elif path.name.startswith(BFINDEX_FILE_PREFIX):
                    continue  # ignore bloom filter index files
                else:
                    warnings.warn('channel scan: found unknown file "{}" '
                                  'in "{}" channel storage hierarchy, skipping'
                                  .format(path, self.base_path.name))

            self.flow_files.sort(key=lambda flow_file: flow_file.name_datetime)

            if self.flow_files:
                vprint('found {} flow files, earliest from {}, latest from {}'
                       .format(len(self.flow_files),
                               self.get_earliest_flow_file().name_datetime,
                               self.get_latest_flow_file().name_datetime),
                       use_prefix=False)
            else:
                vprint('found 0 flow files', use_prefix=False)

        class FlowFile:
            """Flow file representation."""

            def __init__(self, path, parent_channel):
                self.path = path  # pathlib path to the file
                self.parent_channel = parent_channel  # file's parent Channel
                self.name_datetime = None
                self.path_date = None

                self._size = None  # never use directly, always use get_size()

            def __repr__(self):
                return ('{}(name={}, parent_channel={}, name_datetime={}, '
                        'path_date={})'
                        .format(type(self).__name__, self.path.name,
                                self.parent_channel.base_path.name,
                                self.name_datetime, self.path_date))

            def __lt__(self, other):
                return self.name_datetime < other.name_datetime

            def __le__(self, other):
                return self.name_datetime <= other.name_datetime

            def __eq__(self, other):
                return self.name_datetime == other.name_datetime

            def __gt__(self, other):
                return self.name_datetime > other.name_datetime

            def __ge__(self, other):
                return self.name_datetime >= other.name_datetime

            def parse_path(self):
                """Discover date and time from file's path.

                Search for patterns YYYYMMDDHHMM or YYYYMMDDHHMMSS in the
                file's name and if found, convert the match into corresponding
                datetime object. Then check if file's directory hierarchy
                comply with YYYY/MM/DD format.
                """

                match = DATETIME_REGEX.search(self.path.name)
                if not match:
                    raise ValueError('flow file parse path: patterns '
                                     'YYYYMMDDHHMM or YYYYMMDDHHMMSS not '
                                     'found in file\'s "{}" name'
                                     .format(self.path))

                if match.group('second'):
                    strtime_format = STRTIME_FORMAT_YMDHMS
                else:
                    strtime_format = STRTIME_FORMAT_YMDHM
                self.name_datetime = datetime.datetime.strptime(match.group(0),
                                                                strtime_format)
                try:
                    day = int(self.path.parent.name)
                    month = int(self.path.parent.parent.name)
                    year = int(self.path.parent.parent.parent.name)
                    self.path_date = datetime.date(year, month, day)
                except Exception:
                    warnings.warn('flow file parse path: "{}" does not comply '
                                  'with YYYY/MM/DD directory hierarchy'
                                  .format(self.path))
                if self.name_datetime.date() != self.path_date:
                    warnings.warn('flow file parse path: path/name date '
                                  'mismatch, name corresponds to "{}", path '
                                  'corresponds to "{}"'
                                  .format(self.name_datetime, self.path_date))

            def get_size(self):
                """Return size of the flow file in bytes. Lazy."""

                if not self.path.exists():
                    raise FileNotFoundError
                elif not self._size:
                    self._size = os.stat(str(self.path)).st_size
                return self._size

            def unlink(self):
                if args.dry_run:
                    dprint('would unlink "{}"'.format(self.path))
                else:
                    vprint('unlinking "{}"'.format(self.path))
                    self.path.unlink()


class UnlinkedCounters:
    def __init__(self):
        self.files = 0  # number of unlinked files
        self.columns = 0  # number of unlinked columns of files
        self.bytes = 0  # size of all unlinked files in bytes

    def __repr__(self):
        return ('{}(files={}, columns={}, bytes={}'
                .format(type(self).__name__, self.files, self.columns,
                        self.bytes))

    def add(self, other):
        self.files += other.files
        self.columns += other.columns
        self.bytes += other.bytes


###############################################################################
def vprint_formatter(*args, **kwargs):
    """Verbose-enabled print formatter."""
    if 'use_prefix' in kwargs:
        use_prefix = kwargs['use_prefix']
        del(kwargs['use_prefix'])
    else:
        use_prefix = True

    if use_prefix:
        print(OUTPUT_PREFIX, 'VERBOSE:', *args, **kwargs)
    else:
        print(*args, **kwargs)


def vprint_formatter_void(*args, **kwargs):
    """Verbose-disabled print formatter."""
    pass


def myformatwarning(message, category, filename, lineno, line=None):
    return '{} WARNING: {}\n'.format(OUTPUT_PREFIX, str(message))


###############################################################################
def scan_main():
    scan_nodes()


def scan_add_parser(subparsers):
    parser = subparsers.add_parser(
            'scan', help='scan help',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.set_defaults(subcommand_func_main=scan_main)
    parser.set_defaults(subcommand_name='scan')
    return parser


def scan_check_args(args):
    return


def scan_nodes():
    # find node directories in the base path
    vprint('base directory "{}" scan:'.format(base_path_arg), end=' ')
    content_dict = dir_scan(base_path_arg)
    vprint('using dirs: {}, ignoring files: {}'
           .format(content_dict['dirs_n'], content_dict['others_n']),
           use_prefix=False)
    if not content_dict['dirs']:
        raise Exception('node directory lookup: nothing appropriate found in '
                        'the given directory')

    # check if all node directories are placed on the same device
    ref_st_dev = next(iter(content_dict['dirs'])).stat().st_dev
    if not all(ref_st_dev == node_dir.stat().st_dev
               for node_dir in content_dict['dirs']):
        raise Exception('node directory lookup: one or more node directories '
                        'are placed across different devices')

    # scan all node directories and create profiles, recursively
    nodes = []
    for path in content_dict['dirs']:
        node = Node(path)
        node.scan()
        nodes.append(node)

    return nodes


###############################################################################
def delete_main():
    rc = 0

    # check if used disk space is below the given upper bound
    du_before = disk_usage(base_path_arg)
    if du_before[2] < args.du_boundary:
        vprint('nothing to do, used disk space ({:.2f} %) is below the disk '
               'usage boundary ({:.2f} %)'.format(du_before[2],
                                                  args.du_boundary))
        return 0

    # calculate number of files/columns/bytes to unlink
    bytes_cnt_list = []
    if args.bytes_cnt is not None:
        bytes_cnt_list.append(args.bytes_cnt)
    if args.du_lower_by is not None or args.du_lower_to is not None:
        du_lower_list = []
        if args.du_lower_by is not None:
            diff_percent = du_before[2] - args.du_lower_by
            du_lower_list.append(diff_percent if diff_percent > 0 else 0)
        if args.du_lower_to is not None:
            du_lower_list.append(args.du_lower_to)
        max_du_lower_to = max(du_lower_list)

        statvfs = os.statvfs(str(base_path_arg))
        now_used_blocks = statvfs.f_blocks - statvfs.f_bavail
        des_used_blocks = statvfs.f_blocks * (max_du_lower_to / 100)
        diff_blocks = now_used_blocks - des_used_blocks
        if diff_blocks < 0:
            vprint('nothing to do, used disk space ({:.2f} %) is below the '
                   'target disk usage ({:.2f} %)'.format(du_before[2],
                                                         max_du_lower_to))
            return 0
        else:
            bytes_cnt_list.append(diff_blocks * statvfs.f_frsize)
    min_bytes_cnt = min(bytes_cnt_list) if bytes_cnt_list else None

    vprint('going to unlink {} files, {} columns, {}B'
           .format(args.files_cnt if args.files_cnt is not None else '?',
                   args.columns_cnt if args.columns_cnt is not None else '?',
                   int_prefix_str(min_bytes_cnt) if min_bytes_cnt is not None
                   else '?'))

    # get a sorted list (earliest first) of all flow files from all channels
    all_files = []
    all_channels = set()
    for node in scan_nodes():
        for profile in node.profiles:
            for channel in profile.get_channels(recursively=True):
                all_channels.add(channel)
                all_files += channel.flow_files
    all_files.sort()

    # unlink flow files from the list until certain stop condition is met
    target_reached, aligned, dcounters = unlink_earliest(
            all_files, files_cnt=args.files_cnt, columns_cnt=args.columns_cnt,
            bytes_cnt=min_bytes_cnt)
    if not target_reached:
        warnings.warn('all files {} unlinked without reaching any of the stop '
                      'conditions'
                      .format('would be' if args.dry_run else 'were'))

    # align only if it was requested and if at least one file was unlinked
    acounters = UnlinkedCounters()
    if args.align_to and dcounters.files:
        if aligned:
            vprint('already aligned')
        else:
            vprint('aligning to {}'.format(args.align_to))
            acounters = align_column(all_files, args.align_to)

    # cleanup by recursively deleting empty directories
    cleanup_counter = 0
    empty_counter = 0
    if args.cleanup:
        for channel in all_channels:
            cleanup_counter += channel.remove_unused_directories()
            empty_counter += channel.remove_empty_directories()

    print('delete summary:     {} {} file(s), {} column(s), {}B'
          .format('would unlink' if args.dry_run else 'unlinked',
                  dcounters.files, dcounters.columns,
                  int_prefix_str(dcounters.bytes)))
    print('alignment summary:  {} {} file(s), {} column(s), {}B'
          .format('would unlink' if args.dry_run else 'unlinked',
                  acounters.files, acounters.columns,
                  int_prefix_str(acounters.bytes)))
    print('cleanup summary:    {} {} flow dir(s), {} empty dir(s)'
          .format('would remove' if args.dry_run else 'removed',
                  cleanup_counter, empty_counter))
    du_after = disk_usage(base_path_arg)
    print('disk usage summary: '
          'before {}B ({:.2f} %), '
          'after {}B ({:.2f} %), '
          'difference {}B'
          .format(int_prefix_str(du_before[1]), du_before[2],
                  int_prefix_str(du_after[1]), du_after[2],
                  int_prefix_str(du_before[1] - du_after[1])))
    return rc


def delete_add_parser(subparsers):
    parser = subparsers.add_parser(
            'delete', help='delete help',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.set_defaults(subcommand_func_main=delete_main)
    parser.set_defaults(subcommand_name='delete')

    parser.add_argument('-u', '--du-boundary',
                        dest='du_boundary',
                        type=argument_type_percent,
                        default=90,
                        help='Percentual upper boundary of disk usage: do '
                             'nothing unless the current disk usage is '
                             'greater than this value.')
    parser.add_argument('--align-to',
                        dest='align_to',
                        choices=['second', 'minute', 'hour', 'day', 'month',
                                 'year'],
                        default=None,
                        help='Delete zero or more files (additional to the '
                             'files deleted to satisfy the stop condition) to '
                             'ensure alignment. If one or more files from '
                             'certain second/minute/hour/day/month/year is '
                             'deleted, delete also all other files from that '
                             'second/minute/hour/day/month/year.')
    parser.add_argument('--cleanup',
                        dest='cleanup',
                        action='store_true',
                        help='Remove directories with no flow files inside '
                             'channels.')
    parser.add_argument('--no-cleanup',
                        dest='cleanup',
                        action='store_false',
                        help='Don\'t remove directories with no flow files '
                             'inside channels.')
    parser.set_defaults(cleanup=True)

    group1 = parser.add_argument_group(
        title='stop condition arguments',
        description='Use one or more of these arguments to express which and '
                    'how many flow files should be deleted. If the argument '
                    'is not present, its value defaluts to "don\'t care". If '
                    'more than one are present, all of them are evaluated and '
                    'the program returns immediately after whichever is '
                    'satisfied.')
    group1.add_argument('-f', '--files-count',
                        dest='files_cnt',
                        type=argument_type_positive_int,
                        help='Remove specified number of files.')
    group1.add_argument('-c', '--columns-count',
                        dest='columns_cnt',
                        type=argument_type_positive_int,
                        help='Remove specified number of columns.')
    group1.add_argument('-b', '--bytes-count',
                        dest='bytes_cnt',
                        type=argument_type_positive_int,
                        help='Remove specified number of bytes.')
    group1.add_argument('--du-lower-by',
                        dest='du_lower_by',
                        type=argument_type_percent,
                        help='Lower disk usage by the specified percentual '
                             'value.')
    group1.add_argument('--du-lower-to',
                        dest='du_lower_to',
                        type=argument_type_percent,
                        help='Lower disk usage to the specified percentual '
                             'value.')
    return parser


def delete_check_args(args):
    if (args.files_cnt is None and args.columns_cnt is None and args.bytes_cnt
            is None and args.du_lower_by is None and args.du_lower_to is None):
        return 'stop condition is missing'


def unlink_earliest(files, files_cnt=None, columns_cnt=None, bytes_cnt=None):
    # check validity of argument and their combinations
    if files_cnt is None and columns_cnt is None and bytes_cnt is None:
        raise ValueError('one of files_cnt, columns_cnt, or bytes_cnt has to '
                         'be specified')
    if files_cnt is not None and files_cnt < 0:
        raise ValueError('files_cnt has to be a positive integer')
    if columns_cnt is not None and columns_cnt < 0:
        raise ValueError('columns_cnt has to be a positive integer')
    if bytes_cnt is not None and bytes_cnt < 0:
        raise ValueError('bytes_cnt has to be a positive integer')

    target_reached = True
    aligned = False
    counters = UnlinkedCounters()
    while files:
        # test all specified stop conditions
        if files_cnt is not None and counters.files >= files_cnt:
            vprint('required number of unlinked files reached')
            break
        if columns_cnt is not None and counters.columns >= columns_cnt:
            vprint('required number of unlinked columns reached')
            break
        if bytes_cnt is not None and counters.bytes >= bytes_cnt:
            vprint('required number of unlinked bytes reached')
            break

        flow_file = files.pop(0)  # retireve and remove
        counters.files += 1
        # test whether this file is the last in the column
        if (flow_file.name_datetime
                != next(iter(files), Profile.Channel.FlowFile(None, None))
                .name_datetime):
            counters.columns += 1
            aligned = True
        else:
            aligned = False
        counters.bytes += flow_file.get_size()
        flow_file.unlink()
    else:
        target_reached = False

    return (target_reached, aligned, counters)


def align_column(files, resolution):
    counters = UnlinkedCounters()
    if not files:
        return counters

    reference = files[0].name_datetime
    while files:
        if not datetime_equal(reference, files[0].name_datetime, resolution):
            break
        flow_file = files.pop(0)  # retireve and remove
        counters.files += 1
        counters.columns += (flow_file.name_datetime !=
                             next(iter(files),
                                  Profile.Channel.FlowFile(None, None))
                             .name_datetime)
        counters.bytes += flow_file.get_size()
        flow_file.unlink()

    return counters


def datetime_equal(a, b, resolution):
    if resolution == 'year':
        return a.year == b.year
    elif resolution == 'month':
        return a.year == b.year and a.month == b.month
    elif resolution == 'day':
        return a.year == b.year and a.month == b.month and a.day == b.day
    elif resolution == 'hour':
        return (a.year == b.year and a.month == b.month and a.day == b.day
                and a.hour == b.hour)
    elif resolution == 'minute':
        return (a.year == b.year and a.month == b.month and a.day == b.day
                and a.hour == b.hour and a.minute == b.minute)
    elif resolution == 'second':
        return (a.year == b.year and a.month == b.month and a.day == b.day
                and a.hour == b.hour and a.minute == b.minute
                and a.second == b.second)
    else:
        raise ValueError('invalid resolution "{}"'.format(resolution))


def disk_usage(path):
    """
    Return the amount of disk space used on the file system containing given
    path.

    statvfs.f_blocks is size of fs in f_frsize units
    statvfs.f_bavail is number of free blocks for unprivileged users
    """

    path = str(path)  # for backward compatibility with Python < 3.6
    statvfs = os.statvfs(path)
    used_blocks = statvfs.f_blocks - statvfs.f_bavail

    # (block, bytes, percentual)
    return (used_blocks, used_blocks * statvfs.f_frsize,
            used_blocks / statvfs.f_blocks * 100)


def dir_scan(path):
    """
    Wrapper for Path.iterdir(), which returns dictionaty containing separate
    sets for directories (keys dirs and dirs_n) and other content (keys
    others and others_n).
    """
    dir_content = {}
    for file in path.iterdir():
        if not file.is_dir() or file.name.startswith('.'):
            dir_content.setdefault('others', set()).add(file)
            dir_content.setdefault('others_n', set()).add(file.name)
        else:
            dir_content.setdefault('dirs', set()).add(file)
            dir_content.setdefault('dirs_n', set()).add(file.name)

    dir_content.setdefault('dirs')
    dir_content.setdefault('dirs_n')
    dir_content.setdefault('others')
    dir_content.setdefault('others_n')
    return dir_content


def dir_is_empty(path):
    return not any(True for _ in path.iterdir())


def dir_remove(path, recursively=False):
    if recursively:
        if args.dry_run:
            dprint('would remove directory "{}" (recursively)'.format(path))
        else:
            vprint('removing directory "{}" (recursively)'.format(path))
            shutil.rmtree(path)
    else:
        if args.dry_run:
            dprint('would remove directory "{}" (nonrecursively)'.format(path))
        else:
            vprint('removing directory "{}" (nonrecursively)'.format(path))
            path.rmdir()


def min_all(iterable, selector):
        """Return list of all smallest items in an iterable."""
        smallest = min(iterable, key=selector)
        smallest_selected = selector(smallest)
        return [i for i in iterable if selector(i) == smallest_selected]


def int_prefix(num):
    num = int(num)
    if abs(num) < 1000:
        return (num, None)

    order = math.floor(math.log10(abs(num)))
    div, mod = divmod(order, 3)
    new_num = num * (10 ** (-order + mod))
    return (new_num, METRIC_PREFIXES[div - 1])


def int_prefix_str(num, format_string='{0:.2f} {1}'):
    new_num, prefix = int_prefix(num)
    if prefix:
        return format_string.format(new_num, prefix)
    else:
        return str(num) + ' '


###############################################################################
def parse_args():
    ###########################################################################
    # global parser and its arguments
    parser = argparse.ArgumentParser(
        description='Flow file data manager.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('-n', '--dry-run',
                        dest='dry_run',
                        action='store_true',
                        default=False,
                        help='Don\'t actually remove anything, just show what '
                             'would be done.')
    parser.add_argument('-v', '--verbose',
                        dest='verbose',
                        action='store_true',
                        default=False,
                        help='Be verbose.')

    parser.add_argument('base_path',
                        nargs=1,
                        help='Path to the flow storage directory.')

    subparsers = parser.add_subparsers(title='subcommands',
                                       description='valid subcommands',
                                       dest='subcommand_name',
                                       help='additional help')
    subparsers.required = True

    ###########################################################################
    # add parsers for sub-commands and its arguments
    subcommands_setup_dict = {
            'scan': [scan_add_parser, scan_check_args, None],
            'delete': [delete_add_parser, delete_check_args, None],
            }
    for cmd in subcommands_setup_dict:
        subcommands_setup_dict[cmd][2] = \
                subcommands_setup_dict[cmd][0](subparsers)  # call add_parser

    ###########################################################################
    # parse all command-line arguments
    args = parser.parse_args()

    ###########################################################################
    # check arguments of the specified sub-command by calling check_args func.
    err_str = subcommands_setup_dict[args.subcommand_name][1](args)
    if err_str:
        subcommands_setup_dict[args.subcommand_name][2].error(err_str)

    return args


def argument_type_positive_int(value_str):
    try:
        value_int = int(value_str)
    except:
        exc_msg = sys.exc_info()[1]
        raise argparse.ArgumentTypeError(exc_msg)

    if value_int < 0:
        raise argparse.ArgumentTypeError('negative value is not allowed')
    else:
        return value_int


def argument_type_percent(value_str):
    try:
        value_float = float(value_str)
    except:
        exc_msg = sys.exc_info()[1]
        raise argparse.ArgumentTypeError(exc_msg)

    if value_float < 0 or value_float > 100:
        raise argparse.ArgumentTypeError('only real numbers from range '
                                         '[0, 100] are allowed')
    else:
        return value_float


###############################################################################
def lock_release(lock_file_path):
    """ Release file lock. See lock_acquire(). """

    if not lock_file_path.exists():
        raise FileNotFoundError('"{}": lock file does not exist'
                                .format(lock_file_path))
    if not lock_file_path.is_file():
        raise Exception('"{}" does not point to a regular file'
                        .format(lock_file_path))

    with open(str(lock_file_path), mode='r') as lock_file:
        my_pid = os.getpid()
        lock_file_pid = int(lock_file.read().strip())
        if my_pid != lock_file_pid:
            raise Exception('PID of this process ({}) differs from PID in '
                            'the lock file ({})'.format(my_pid, lock_file_pid))
    lock_file_path.unlink()


def lock_acquire(lock_file_path):
    """ Acquire file lock.

    Basic and naive protection againts multiple instances of this program
    running at the same time. Work on single machine but also on cluster if
    lock_file_path pointing to a shared storage.

    lock_release is registered as a lock-cleanup functions by the atexit
    module. This functions is automatically executed upon normal interpreter
    termination. It is not called when the program is killed by a signal not
    handled by Python, when a Python fatal internal error is detected, or when
    os._exit() is called.
    """

    if lock_file_path.exists():
        raise FileExistsError('"{}": lock file already exists'
                              .format(lock_file_path))

    with open(str(lock_file_path), mode='w') as lock_file:
        lock_file.write(str(os.getpid()) + '\n')
    atexit.register(lock_release, lock_file_path)


###############################################################################
if __name__ == "__main__":
    # set my format for warning print function
    warnings.formatwarning = myformatwarning

    ########################################
    # parse command line arguments
    args = parse_args()
    # if args.subcommand_name != 'delete':
    #     raise NotImplementedError('sub-command "{}" is not implemented yet'
    #                               .format(args.subcommand_name))

    # define a verbose print function and a dry run print function
    vprint = vprint_formatter if args.verbose else vprint_formatter_void
    dprint = print

    # check validity of a given base path
    base_path_arg = Path(args.base_path[0])
    if not base_path_arg.exists():
        raise FileNotFoundError('"{}" does not exist'.format(base_path_arg))
    if not base_path_arg.is_dir():
        raise NotADirectoryError('"{}" does not point to a directory'
                                 .format(base_path_arg))

    ########################################
    # acquire file lock to prevent multiple instances running simultaneously
    lock_acquire(Path(base_path_arg, LOCK_FILE_NAME))

    ########################################
    # do the real work
    sys.exit(args.subcommand_func_main())
