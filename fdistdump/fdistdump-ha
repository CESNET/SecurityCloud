#!/usr/bin/env python3

# author: Jan Wrona, wrona@cesnet.cz

# Copyright (C) 2016 CESNET
#
# LICENSE TERMS
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in
#    the documentation and/or other materials provided with the
#    distribution.
# 3. Neither the name of the Company nor the names of its contributors
#    may be used to endorse or promote products derived from this
#    software without specific prior written permission.
#
# ALTERNATIVELY, provided that this notice is retained in full, this
# product may be distributed under the terms of the GNU General Public
# License (GPL) version 2 or later, in which case the provisions
# of the GPL apply INSTEAD OF those given above.
#
# This software is provided ``as is'', and any express or implied
# warranties, including, but not limited to, the implied warranties of
# merchantability and fitness for a particular purpose are disclaimed.
# In no event shall the company or contributors be liable for any
# direct, indirect, incidental, special, exemplary, or consequential
# damages (including, but not limited to, procurement of substitute
# goods or services; loss of use, data, or profits; or business
# interruption) however caused and on any theory of liability, whether
# in contract, strict liability, or tort (including negligence or
# otherwise) arising in any way out of the use of this software, even
# if advised of the possibility of such damage.


import subprocess
import re
import time
import datetime
import os
import signal
import argparse
import warnings
import shutil
import shlex


MONITOR_INTERVAL = 1  # seconds
DOWN_LIMIT = datetime.timedelta(seconds=5)

binaries = {
    'fdistdump': 'fdistdump',
    'mpi': 'mpiexec',
    'node': 'crm_node',
    'mon': 'crm_mon',
    'attribute': 'crm_attribute'
}


###############################################################################
def myformatwarning(message, category, filename, lineno, line=None):
    return 'Warning: {}\n'.format(str(message))


def get_nodes_names():
    """
    crm_node: tool for displaying low-level node information
        --list: print all known members (past and present) of this cluster

    output:
        <ID> <NAME> member
        ...

    return:
        list of tuples (integer ID, string name)
    """

    args = [binaries['node'], '--list']
    proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE, universal_newlines=True)
    stdout_str, stderr_str = proc.communicate()
    ret_code = proc.wait()

    if ret_code != 0 or stderr_str:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: ' +
                   str(ret_code) + '\nstderr:' + stderr_str)
        raise subprocess.SubprocessError(err_str)

    node_list = []
    for match in re.findall(r'(\d+)\s+(\S+)', stdout_str):
        node_list.append((int(match[0]), match[1]))

    return node_list


def get_nodes_status():
    """
    crm_mon: provides a summary of cluster's current state (online,
             offline, standby, ...)
        --group-by-node: group resources by node
        --hide-headers:  hide all headers (good for parsing)
        --show-detail:   show more details (node IDs, individual clone
                         instances)
        --one-shot:      display the cluster status once on the console
                         and exit (no ncurses)

    output:
        Node <NAME> (<ID>): <STATUS>
        ...

    return:
        list of tuples (integer ID, string status)
    """

    args = [binaries['mon'], '--group-by-node', '--hide-headers',
            '--show-detail', '--one-shot']
    proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE, universal_newlines=True)
    stdout_str, stderr_str = proc.communicate()
    ret_code = proc.wait()

    if ret_code != 0 or stderr_str:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: '
                   + str(ret_code) + '\nstderr:' + stderr_str)
        raise subprocess.SubprocessError(err_str)

    nodes_status = []
    for match in re.findall(r'Node\s+\S+\s+\((\d+)\):\s+(\w+)', stdout_str):
        nodes_status.append((int(match[0]), match[1]))

    return nodes_status


def get_cluster_attribute(attribute_name, node_name=None):
    """
    crm_attribute: manage node's attributes and cluster options
        --query:     query the current value of the attribute/option
        --name=NAME: name of the attribute/option to operate on
        --node=NODE: get an attribute for the named node (instead of a
                     cluster option)

    output:
        scope=nodes  name=<ATTRIBUTE> value=<NODE>

    return:
        attribute value as string or None if no such attribute is set
    """

    args = [binaries['attribute'], '--query',
            '--name={}'.format(attribute_name)]
    if node_name:
        args.append('--node={}'.format(node_name))
    proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE, universal_newlines=True)
    stdout_str, stderr_str = proc.communicate()
    ret_code = proc.wait()

    if ret_code != 0 or stderr_str:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: '
                   + str(ret_code) + '\nstderr:' + stderr_str)
        raise subprocess.SubprocessError(err_str)

    match = re.fullmatch(r'scope={}\s+name={}\s+value=(\S+)\n'.format('nodes'
                         if node_name else 'crm_config', attribute_name),
                         stdout_str)
    if not match:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nbad output:\n'
                   + stdout_str)
        raise subprocess.SubprocessError(err_str)

    return match.group(1)


def load_nodes_dict():
    """
    {
        ID: {
                'name': NAME,
                'successor': ID (only for subcollectors)
            }

        ID: ...
    }
    """

    nodes_dict = {}
    id_names = get_nodes_names()
    ids = set()

    for id, name in id_names:
        nodes_dict[id] = {'name': name}
        ids.add(id)

    for id, name in id_names:
        succ_id = get_cluster_attribute('successor', name)
        if succ_id is None:
            continue  # no successor attribute means node is not subcollector

        succ_id = int(succ_id)
        if succ_id not in ids:
            raise KeyError('node "{}" has invalid successor id "{}".'.format(
                           id, succ_id))
        else:
            nodes_dict[id]['successor'] = succ_id

    return nodes_dict


def load_setup_dict():
    """Return dictionary with certain cluster parameters."""

    setup_dict = {}

    for attribute in ['flow-primary-brick', 'flow-backup-brick']:
        value = get_cluster_attribute(attribute)
        if value is None:
            raise Exception('cluster attribute "{}" is not defined.'
                            .format(attribute))
        else:
            setup_dict[attribute] = value

    return setup_dict


def get_master_set(nodes_dict):
    """Return set of master nodes."""

    masters = set()
    for key, value in nodes_dict.items():
        if 'successor' not in value:
            masters.add(key)

    return masters


def get_subcollector_set(nodes_dict):
    """Return set of subcollector nodes."""

    subcollectors = set()
    for key, value in nodes_dict.items():
        if 'successor' in value:
            subcollectors.add(key)

    return subcollectors


def get_online_set():
    """Return set of online nodes."""

    online_set = set()
    for id, status in get_nodes_status():
        if status == 'online':
            online_set.add(id)

    return online_set


def graph_check_validity(nodes_dict):
    """Check validity of the topology graph."""

    subcoll_set = get_subcollector_set(nodes_dict)

    if len(subcoll_set) == 0:
        print('Error: empty subcollector set.')
        return False

    first_id = node_id = subcoll_set.pop()
    succ_id = nodes_dict[node_id]['successor']

    if node_id == succ_id:
        warnings.warn('self-loop doesn\'t make sense.')
        return True

    while succ_id in subcoll_set:
        subcoll_set.remove(succ_id)
        node_id = succ_id
        succ_id = nodes_dict[node_id]['successor']

    if succ_id != first_id or len(subcoll_set) != 0:
        print('Error: invalid subcollector graph.')
        return False
    else:
        return True


def graph_print(nodes_dict):
    """Print VALID topology graph."""

    subcoll_set = get_subcollector_set(nodes_dict)

    # find first node
    curr_id = subcoll_set.pop()
    succ_id = nodes_dict[curr_id]['successor']
    graph_str = '--> ' + nodes_dict[curr_id]['name']

    while succ_id in subcoll_set:
        subcoll_set.remove(succ_id)
        curr_id = succ_id
        succ_id = nodes_dict[curr_id]['successor']
        graph_str += ' --> ' + nodes_dict[curr_id]['name']

    bottom_line = '_' * len(graph_str)
    graph_str += '--'
    print(graph_str)
    print('|' + bottom_line + '|')


def construct_hosts_and_paths(nodes_dict, setup_dict):
    """
    """

    master_name = ''
    slave_names = []
    paths = []

    online_set = get_online_set()
    master_set = get_master_set(nodes_dict)
    subcollector_set = get_subcollector_set(nodes_dict)

    # choose best master node
    for master in master_set:
        if master in online_set:
            # we have a dedicated master node
            master_name = nodes_dict[master]['name']
            break
    else:
        # no dedicated master found, choose an arbitrary subcollector node as a
        # master node
        for sub in subcollector_set:
            if sub in online_set:
                    master_name = nodes_dict[sub]['name']
                    break
        else:  # no master found at all
            raise Exception('no node suitable for master role found.')

    # at least one online subcollector is mandatory
    if subcollector_set.isdisjoint(online_set):
        raise Exception('no online subcollector found.')

    # add all online subcollector nodes as slaves
    for sub in subcollector_set:
        if sub in online_set:
            slave_names.append(nodes_dict[sub]['name'])

    # handle all not online subcollector nodes
    for sub in subcollector_set:
        if sub not in online_set:
            sub_name = nodes_dict[sub]['name']
            succ = nodes_dict[sub]['successor']
            succ_name = nodes_dict[succ]['name']

            warn_msg = 'node {} ({}) isn\'t online, '.format(sub, sub_name)
            if succ in online_set:
                # successor is online, still change to have a complete data set
                succ_idx = slave_names.index(succ_name) + 1
                fdistdump_conv_spec = '%{}:'.format(succ_idx)
                succ_path = os.path.join(
                    fdistdump_conv_spec + setup_dict['flow-backup-brick'],
                    sub_name)
                paths.append(succ_path)
                warn_msg += 'using successor {} ({}) as a backup.' \
                    .format(succ, succ_name)
                warnings.warn(warn_msg)
            else:
                # successor is offline, data set will be incomplete
                warn_msg += 'its successor {} ({}) also isn\'t online. ' \
                    'Data set for this job is incomplete!' \
                    .format(succ, succ_name)
                warnings.warn(warn_msg)

    # prepend master node to slaves and append global primary path
    slave_names.insert(0, master_name)
    paths.append(os.path.join(setup_dict['flow-primary-brick'], '%h'))

    return (slave_names, paths)


def signal_handler(signal, frame):
    verbose_print('Received signal', signal)
    # TODO: disable restart

    if 'mpi_job_proc' not in globals() or mpi_job_proc is None:
        warnings.warn('job has not been started yet.')
        return

    # When signal is sent to the process group (which is the same for the
    # fdistdump-ha and fdistdump), MPI process manager will terminate the job
    # in a usual way, but it can take a while (therefore the sleep()).
    warnings.warn('waiting for job self-termination.')
    for i in range(5):
        time.sleep(1)
        if mpi_job_proc.poll() is not None:
            return

    # When signal is sent only to the fdistdump-ha process or MPI process
    # manager for some reason couldn't terminate it by itself, its our
    # responsibility to terminate the job.
    warnings.warn('terminating job.')
    mpi_job_proc.terminate()
    mpi_job_proc.wait(5)


def run():
    # load current cluster and nodes setup
    setup_dict = load_setup_dict()
    nodes_dict = load_nodes_dict()

    # check the topology graph validity
    if not graph_check_validity(nodes_dict):
        return 1

    # construct hosts and appropriate paths based on the current cluster status
    hosts, base_paths = construct_hosts_and_paths(nodes_dict, setup_dict)
    mpi_hosts = ','.join(hosts)  # comma separated hostnames
    fdistdump_paths = []  # space separated paths
    for base_path in base_paths:
        fdistdump_paths += [shlex.quote(os.path.join(base_path, user_path))
                            for user_path in user_paths]

    # get reference set of online nodes and run the job
    subprocess_args = (mpi_args + [mpi_hosts] + priority_args + fdistdump_args
                       + fdistdump_paths)
    verbose_print("launching command:", ' '.join([shlex.quote(arg)
                  for arg in subprocess_args]))
    online_ref_set = get_online_set()
    global mpi_job_proc
    mpi_job_proc = subprocess.Popen(subprocess_args)

    # start monitoring loop
    state_cur = 'reference'
    online_cur_set = online_ref_set
    node_down_for = 0  # seconds
    while mpi_job_proc.poll() is None:
        online_prev_set = online_cur_set
        online_cur_set = get_online_set()
        cur_ts = datetime.datetime.now()

        if state_cur != 'reference':
            ts_delta = cur_ts - transition_ts
            if ts_delta >= DOWN_LIMIT:
                warnings.warn('no-in-referece-state time limit exceeded, '
                              'terminating job.')
                mpi_job_proc.terminate()
                mpi_job_proc.wait(5)
                break

        # compare current state to the previous state
        if online_cur_set != online_prev_set:
            # one or more nodes went online, offline or combination
            online_superset = online_cur_set - online_prev_set
            offline_superset = online_prev_set - online_cur_set

            for node in online_superset:
                warnings.warn('node {} has upgraded to online state.'
                              .format(nodes_dict[node]['name']))
            for node in offline_superset:
                warnings.warn('node {} has degraded from online state.'
                              .format(nodes_dict[node]['name']))

        # compare current state to the reference state
        if online_cur_set != online_ref_set:
            # one or more nodes is online, offline or combination
            online_superset = online_cur_set - online_ref_set
            offline_superset = online_ref_set - online_cur_set

            # FSM transition
            if online_superset:
                state_new = 'online superset'
            elif offline_superset:
                state_new = 'offline superset'
            elif online_superset and offline_superset:
                state_new = 'mixed'
        else:
            state_new = 'reference'

        if state_cur != state_new:
            if state_cur == 'reference':
                transition_ts = datetime.datetime.now()

            warnings.warn('monitoring FSM going from "{}" state to the "{}" '
                          'state.'.format(state_cur, state_new))
            state_cur = state_new

        time.sleep(MONITOR_INTERVAL)

    # process finished
    if mpi_job_proc.returncode != 0:
        warnings.warn('MPI process manager returned non-zero exit code {}.'
                      .format(mpi_job_proc.returncode))

    rc = mpi_job_proc.returncode
    mpi_job_proc = None
    return rc

###############################################################################
if __name__ == "__main__":
    ########################################
    # attach handler to SIGINT and SIGTERM for gracefull job termination
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # set my format for warning print function
    warnings.formatwarning = myformatwarning

    # On some distros (Fedora, RHEL), the binaries MUST be suffixed with
    # $MPI_SUFFIX (e.g. _openmpi for Open MPI, _mpich for MPICH, ...)
    mpi_suffix = os.environ.get('MPI_SUFFIX')
    if mpi_suffix is not None:
        binaries['fdistdump'] += mpi_suffix

    # check presence of used binaries
    for key, value in binaries.items():
        if shutil.which(value) is None:
            raise OSError('executable "{}" not found.'.format(value))

    ########################################
    # process command line arguments
    # set and parse
    parser = argparse.ArgumentParser(description='fdistdump(1) wrapper with '
                                     'support for high-availability using '
                                     'Corosync/Pacemaker stack.')
    parser.add_argument('-m', '--mpi-args', dest='user_mpi_args',
                        help='Arguments passed to the mpiexec(1) command.')
    parser.add_argument('-f', '--fdistdump-args', dest='user_fdistdump_args',
                        help='Arguments passed to the fdistdump(1) command.')

    parser.add_argument('-p', '--priority', dest='priority', choices=['low',
                        'normal', 'high'], help='Run fdistdump with adjusted '
                        'niceness and I/O scheduling class. For more '
                        'information see nice(1) and ionice(1).')
    parser.add_argument('-v', '--verbose', dest='verbose', action='store_true',
                        help='Be verbose.')

    parser.add_argument('path', nargs='+', help='Path to your flow '
                        'file/directory relative to the "flow-primary-brick" '
                        'Pacemaker property, e.g. path will be "2016/07/" if '
                        'you want to process all flow files from July 2016.')
    args = parser.parse_args()

    # pick only relative paths from user arguments
    user_paths = []
    for path in args.path:
        if path[0] == '/':
            warnings.warn('skipping absolute path "{}".'.format(path))
        else:
            user_paths.append(path)
    if len(user_paths) == 0:
        raise ValueError('no valid path.')

    # handle priority classes
    # don't change anything if class not set or set to normal
    priority_args = []
    if args.priority == 'low':
        priority_args = ['nice', '-n', '10', 'ionice', '-c', '3']
    elif args.priority == 'high':
        priority_args = ['nice', '-n', '-10', 'ionice', '-c', '1']

    # construct fdistdump arguments
    fdistdump_args = [binaries['fdistdump']]
    if args.user_fdistdump_args:
        fdistdump_args += shlex.split(args.user_fdistdump_args)

    # construct MPI args, hosts are added later based on their current status
    mpi_args = [binaries['mpi'], '-wdir', '/tmp/', '-host']
    if args.user_mpi_args:
        mpi_args += shlex.split(args.user_mpi_args)

    # define verbose print function
    verbose_print = print if args.verbose else lambda *a, **k: None

    ########################################
    mpi_job_proc = None  # make the subprocess object global for signal handler
    rc = run()
    exit(rc)
