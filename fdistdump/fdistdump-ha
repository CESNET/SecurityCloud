#!/usr/bin/env python3

# author: Jan Wrona, wrona@cesnet.cz

# Copyright (C) 2016 CESNET
#
# LICENSE TERMS
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in
#    the documentation and/or other materials provided with the
#    distribution.
# 3. Neither the name of the Company nor the names of its contributors
#    may be used to endorse or promote products derived from this
#    software without specific prior written permission.
#
# ALTERNATIVELY, provided that this notice is retained in full, this
# product may be distributed under the terms of the GNU General Public
# License (GPL) version 2 or later, in which case the provisions
# of the GPL apply INSTEAD OF those given above.
#
# This software is provided ``as is'', and any express or implied
# warranties, including, but not limited to, the implied warranties of
# merchantability and fitness for a particular purpose are disclaimed.
# In no event shall the company or contributors be liable for any
# direct, indirect, incidental, special, exemplary, or consequential
# damages (including, but not limited to, procurement of substitute
# goods or services; loss of use, data, or profits; or business
# interruption) however caused and on any theory of liability, whether
# in contract, strict liability, or tort (including negligence or
# otherwise) arising in any way out of the use of this software, even
# if advised of the possibility of such damage.


import sys
import os
import argparse
import subprocess
import re
import time
import datetime
import threading
import signal
import warnings
import shutil
import shlex


OUTPUT_PREFIX = os.path.basename(__file__).upper() if '__file__' in globals() \
    else 'FDISTDUMP-HA'

binaries = {
    'fdistdump': 'fdistdump',
    'mpi': 'mpiexec',
    'node': 'crm_node',
    'mon': 'crm_mon',
    'attribute': 'crm_attribute'
}


###############################################################################
# exceptions
class Error(Exception):
    """Base class for exceptions in this module."""
    pass


class GraphError(Error):
    """Exception raised for errors in the graph.

    Attributes:
        message -- explanation of the error
    """

    def __init__(self, message):
        self.message = message


###############################################################################
def myformatverbose(*args, **kwargs):
    print(OUTPUT_PREFIX, 'VERBOSE:', *args, **kwargs)


def myformatwarning(message, category, filename, lineno, line=None):
    return '{} WARNING: {}\n'.format(OUTPUT_PREFIX, str(message))


def get_nodes_names():
    """
    crm_node: tool for displaying low-level node information
        --list: print all known members (past and present) of this cluster

    output:
        <ID> <NAME> member
        ...

    return:
        list of tuples (integer ID, string name)
    """

    args = [binaries['node'], '--list']
    proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE, universal_newlines=True)
    stdout_str, stderr_str = proc.communicate()
    ret_code = proc.wait()

    if ret_code != 0 or stderr_str:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: ' +
                   str(ret_code) + '\nstderr:' + stderr_str)
        raise subprocess.SubprocessError(err_str)

    node_list = []
    for match in re.findall(r'(\d+)\s+(\S+)', stdout_str):
        node_list.append((int(match[0]), match[1]))

    return node_list


def get_nodes_status():
    """
    crm_mon: provides a summary of cluster's current state (online,
             offline, standby, ...)
        --group-by-node: group resources by node
        --hide-headers:  hide all headers (good for parsing)
        --show-detail:   show more details (node IDs, individual clone
                         instances)
        --one-shot:      display the cluster status once on the console
                         and exit (no ncurses)

    output:
        Node <NAME> (<ID>): <STATUS>
        ...

    return:
        list of tuples (integer ID, string status)
    """

    args = [binaries['mon'], '--group-by-node', '--hide-headers',
            '--show-detail', '--one-shot']
    proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE, universal_newlines=True)
    stdout_str, stderr_str = proc.communicate()
    ret_code = proc.wait()

    if ret_code != 0 or stderr_str:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: '
                   + str(ret_code) + '\nstderr:' + stderr_str)
        raise subprocess.SubprocessError(err_str)

    nodes_status = []
    for match in re.findall(r'Node\s+\S+\s+\((\d+)\):\s+(\w+)', stdout_str):
        nodes_status.append((int(match[0]), match[1]))

    return nodes_status


def get_cluster_attribute(attribute_name, node_name=None):
    """
    crm_attribute: manage node's attributes and cluster options
        --query:     query the current value of the attribute/option
        --name=NAME: name of the attribute/option to operate on
        --node=NODE: get an attribute for the named node (instead of a
                     cluster option)

    output:
        scope=nodes  name=<ATTRIBUTE> value=<NODE>

    return:
        attribute value as string or None if no such attribute is set
    """

    args = [binaries['attribute'], '--query',
            '--name={}'.format(attribute_name)]
    if node_name:
        args.append('--node={}'.format(node_name))
    proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE, universal_newlines=True)
    stdout_str, stderr_str = proc.communicate()
    ret_code = proc.wait()

    if ret_code != 0 or stderr_str:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: '
                   + str(ret_code) + '\nstderr:' + stderr_str)
        raise subprocess.SubprocessError(err_str)

    match = re.fullmatch(r'scope={}\s+name={}\s+value=(\S+)\n'.format('nodes'
                         if node_name else 'crm_config', attribute_name),
                         stdout_str)
    if not match:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nbad output:\n'
                   + stdout_str)
        raise subprocess.SubprocessError(err_str)

    return match.group(1)


def load_nodes_dict():
    """
    {
        ID: {
                'name': NAME,
                'successor': ID (only for subcollectors)
            }

        ID: ...
    }
    """

    nodes_dict = {}
    id_names = get_nodes_names()
    ids = set()

    for id, name in id_names:
        nodes_dict[id] = {'name': name}
        ids.add(id)

    for id, name in id_names:
        succ_id = get_cluster_attribute('successor', name)
        if succ_id is None:
            continue  # no successor attribute means node is not subcollector

        succ_id = int(succ_id)
        if succ_id not in ids:
            raise KeyError('node "{}" has invalid successor id "{}".'.format(
                           id, succ_id))
        else:
            nodes_dict[id]['successor'] = succ_id

    return nodes_dict


def load_setup_dict():
    """Return dictionary with certain cluster parameters."""

    setup_dict = {}

    for attribute in ['flow-primary-brick', 'flow-backup-brick']:
        value = get_cluster_attribute(attribute)
        if value is None:
            raise Exception('cluster attribute "{}" is not defined.'
                            .format(attribute))
        else:
            setup_dict[attribute] = value

    return setup_dict


def get_master_set(nodes_dict):
    """Return set of master nodes."""

    masters = set()
    for key, value in nodes_dict.items():
        if 'successor' not in value:
            masters.add(key)

    return masters


def get_subcollector_set(nodes_dict):
    """Return set of subcollector nodes."""

    subcollectors = set()
    for key, value in nodes_dict.items():
        if 'successor' in value:
            subcollectors.add(key)

    return subcollectors


def get_online_set():
    """Return set of online nodes."""

    online_set = set()
    for id, status in get_nodes_status():
        if status == 'online':
            online_set.add(id)

    return online_set


def graph_check_validity(nodes_dict):
    """Check validity of the topology graph."""

    subcoll_set = get_subcollector_set(nodes_dict)

    if len(subcoll_set) == 0:
        raise GraphError('empty subcollector set')

    first_id = node_id = subcoll_set.pop()
    succ_id = nodes_dict[node_id]['successor']

    if node_id == succ_id:
        warnings.warn('self-loop doesn\'t make sense')
        return

    while succ_id in subcoll_set:
        subcoll_set.remove(succ_id)
        node_id = succ_id
        succ_id = nodes_dict[node_id]['successor']

    if succ_id != first_id or len(subcoll_set) != 0:
        raise GraphError('invalid subcollector graph')


def graph_print(nodes_dict):
    """Print VALID topology graph."""

    subcoll_set = get_subcollector_set(nodes_dict)

    # find first node
    curr_id = subcoll_set.pop()
    succ_id = nodes_dict[curr_id]['successor']
    graph_str = '--> ' + nodes_dict[curr_id]['name']

    while succ_id in subcoll_set:
        subcoll_set.remove(succ_id)
        curr_id = succ_id
        succ_id = nodes_dict[curr_id]['successor']
        graph_str += ' --> ' + nodes_dict[curr_id]['name']

    bottom_line = '_' * len(graph_str)
    graph_str += '--'
    print(graph_str)
    print('|' + bottom_line + '|')


def construct_hosts_and_paths(nodes_dict, setup_dict):
    """
    """

    master_name = ''
    slave_names = []
    paths = []

    online_set = get_online_set()
    master_set = get_master_set(nodes_dict)
    subcollector_set = get_subcollector_set(nodes_dict)

    # choose best master node
    for master in master_set:
        if master in online_set:
            # we have a dedicated master node
            master_name = nodes_dict[master]['name']
            break
    else:
        # no dedicated master found, choose an arbitrary subcollector node as a
        # master node
        for sub in subcollector_set:
            if sub in online_set:
                    master_name = nodes_dict[sub]['name']
                    break
        else:  # no master found at all
            raise Exception('no node suitable for master role found.')

    # at least one online subcollector is mandatory
    if subcollector_set.isdisjoint(online_set):
        raise Exception('no online subcollector found.')

    # add all online subcollector nodes as slaves
    for sub in subcollector_set:
        if sub in online_set:
            slave_names.append(nodes_dict[sub]['name'])

    # handle all not online subcollector nodes
    for sub in subcollector_set:
        if sub not in online_set:
            sub_name = nodes_dict[sub]['name']
            succ = nodes_dict[sub]['successor']
            succ_name = nodes_dict[succ]['name']

            warn_msg = 'node {} ({}) isn\'t online, '.format(sub, sub_name)
            if succ in online_set:
                # successor is online, still change to have a complete data set
                succ_idx = slave_names.index(succ_name) + 1
                fdistdump_conv_spec = '%{}:'.format(succ_idx)
                succ_path = os.path.join(
                    fdistdump_conv_spec + setup_dict['flow-backup-brick'],
                    sub_name)
                paths.append(succ_path)
                warn_msg += 'using successor {} ({}) as a backup' \
                    .format(succ, succ_name)
                warnings.warn(warn_msg)
            else:
                # successor is offline, data set will be incomplete
                warn_msg += 'its successor {} ({}) also isn\'t online. ' \
                    'Data set for this job is incomplete' \
                    .format(succ, succ_name)
                warnings.warn(warn_msg)

    # prepend master node to slaves and append global primary path
    slave_names.insert(0, master_name)
    paths.append(os.path.join(setup_dict['flow-primary-brick'], '%h'))

    return (slave_names, paths)


def terminate_job(job_popen, timeout):
    verbose_print('sending SIGTERM')
    job_popen.terminate()
    try:
        job_popen.wait(timeout)
    except subprocess.TimeoutExpired:
        verbose_print('SIGTERM did not work. Sending SIGKILL')
        job_popen.kill()
        try:
            job_popen.wait(timeout)
        except subprocess.TimeoutExpired:
            verbose_print('SIGKILL did not work, giving up')
        else:
            verbose_print('SIGKILL has terminated the job')
    else:
        verbose_print('SIGTERM has terminated the job')


def signal_handler(signal, frame):
    """
    When signal is sent to the process group (which is the same for the
    fdistdump-ha and fdistdump), MPI process manager will terminate the job
    in a usual way, but it can take a while (therefore the first wait).

    But when signal is sent only to the fdistdump-ha process or MPI process
    manager for some reason couldn't terminate the job by itself, its our
    responsibility to terminate the job -- first by SIGTERM, then by SIGKILL.
    """

    warnings.warn('received signal {}, terminating the job and exitting as '
                  'soon as possible'.format(signal))
    timeout = 5

    if ('mpi_job_proc' not in globals()
            or mpi_job_proc is None
            or mpi_job_proc.poll() is not None):
        verbose_print('job is not running, exiting immediately')
    else:
        verbose_print('job is running, waiting {} seconds for job to '
                      'self-terminate'.format(timeout))
        try:
            mpi_job_proc.wait(timeout)
        except subprocess.TimeoutExpired:
            verbose_print('job did not self-terminate')
            terminate_job(mpi_job_proc, timeout)
        else:
            verbose_print('job has self-terminated')

    sys.exit()  # only raises the SystemExit exception


class ClusterHealthMonitor:
    """Cluster health monitoring class."""

    def __init__(self, online_ref_set, nodes_dict):
        self.state = self.State()
        self.online_ref_set = online_ref_set
        self.online_set = online_ref_set
        self.nodes_dict = nodes_dict
        self.termination_timer = threading.Timer(
            args.termination_timeout.total_seconds(), self._terminate)
        self.terminated = False

    def __del(self):
        self.termination_timer.cancel()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.termination_timer.cancel()

    def _terminate(self):
        warnings.warn('{}: time spent in the denied state exceeded its limit, '
                      'terminating the job'.format(type(self).__name__))
        terminate_job(mpi_job_proc, 5)
        self.terminated = True

    def update(self, online_set_new):
            # compare the current online node set to the previous one
            if online_set_new != self.online_set:
                # one or more nodes went online, offline or combination
                online_superset = online_set_new - self.online_set
                offline_superset = self.online_set - online_set_new

                for node in online_superset:
                    warnings.warn(
                        '{}: node {} has upgraded to the online status'
                        .format(type(self).__name__,
                                self.nodes_dict[node]['name']))
                for node in offline_superset:
                    warnings.warn(
                        '{}: node {} has degraded from the online status'
                        .format(type(self).__name__,
                                self.nodes_dict[node]['name']))

            # compare the current online node set to the reference one
            online_superset = online_set_new - self.online_ref_set
            offline_superset = self.online_ref_set - online_set_new
            state_new = self.State(bool(online_superset),
                                   bool(offline_superset))

            # check if the state has changed
            if self.state != state_new:
                state_status = 'denied' if \
                    self.state.is_denied(args.allowed_state) else 'allowed'
                state_new_status = 'denied' if \
                    state_new.is_denied(args.allowed_state) else 'allowed'

                warnings.warn('{}: transition from the {} state <{}> to the '
                              '{} state <{}>'.format(type(self).__name__,
                                                     state_status, self.state,
                                                     state_new_status,
                                                     state_new))
                if state_new.is_denied(args.allowed_state):
                    # transition to the denied state, start the timer
                    if not self.termination_timer.is_alive():
                        self.termination_timer.start()
                else:
                    # transition to the allowed state, cancel the timer
                    self.termination_timer.cancel()
                    self.termination_timer = threading.Timer(
                        args.termination_timeout.total_seconds(),
                        self._terminate)

            self.state.update(state_new)
            self.online_set = online_set_new

    class State:
        """Class representing state of the ClusterHealthMonitor."""

        def __init__(self, online=False, offline=False):
            self._online = bool(online)
            self._offline = bool(offline)

        def __eq__(self, other):
            return (self._online == other._online
                    and self._offline == other._offline)

        def __or__(self, other):
            return (self._online == other._online
                    or self._offline == other._offline)

        def __str__(self):
            if self._online and self._offline:
                return 'superset of both online and offline nodes'
            elif self._online:
                return 'superset of online nodes'
            elif self._offline:
                return 'superset of offline nodes'
            else:
                return 'reference'

        def update(self, other):
            self._online = other._online
            self._offline = other._offline

        def is_denied(self, other):
            return (self._online and not other._online
                    or self._offline and not other._offline)

    class ClusterHealthMonitorError(Error):
        """General ClusterHealthMonitor exception.

        Attributes:
            message -- explanation of the error
        """

        def __init__(self, message):
            self.message = message


def run_job():
    # load current cluster and nodes setup
    setup_dict = load_setup_dict()
    nodes_dict = load_nodes_dict()

    # check the topology graph validity
    graph_check_validity(nodes_dict)

    # construct hosts and appropriate paths based on the current cluster status
    hosts, base_paths = construct_hosts_and_paths(nodes_dict, setup_dict)
    mpi_hosts = ','.join(hosts)  # comma separated hostnames
    fdistdump_paths = []  # space separated paths
    for base_path in base_paths:
        fdistdump_paths += [shlex.quote(os.path.join(base_path, user_path))
                            for user_path in user_paths]

    # get reference set of online nodes and run the job
    subprocess_args = (mpi_args + [mpi_hosts] + priority_args + fdistdump_args
                       + fdistdump_paths)
    verbose_print('launching command:', ' '.join([shlex.quote(arg)
                  for arg in subprocess_args]))
    global mpi_job_proc
    mpi_job_proc = subprocess.Popen(subprocess_args)

    # start monitoring loop
    with ClusterHealthMonitor(get_online_set(), nodes_dict) as chm:
        while mpi_job_proc.poll() is None:
            chm.update(get_online_set())
            time.sleep(args.health_monitoring_interval.total_seconds())
        else:
            if chm.terminated:
                raise ClusterHealthMonitor.ClusterHealthMonitorError(
                    'job terminated by the ClusterHealthMonitor')

    # process has finished
    return mpi_job_proc.returncode


def argument_type_positive_int(value_str):
    try:
        value_int = int(value_str)
    except:
        exc_msg = sys.exc_info()[1]
        raise argparse.ArgumentTypeError(exc_msg)

    if value_int < 0:
        raise argparse.ArgumentTypeError('negative value is not allowed')
    else:
        return value_int


def argument_type_timedelta(seconds_str):
    seconds_int = argument_type_positive_int(seconds_str)
    try:
        delta = datetime.timedelta(seconds=seconds_int)
    except:
        exc_msg = sys.exc_info()[1]
        raise argparse.ArgumentTypeError(exc_msg)

    return delta

###############################################################################
if __name__ == "__main__":
    ########################################
    # attach handler to SIGINT and SIGTERM for gracefull job termination
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # set my format for warning print function
    warnings.formatwarning = myformatwarning

    # On some distros (Fedora, RHEL), the binaries MUST be suffixed with
    # $MPI_SUFFIX (e.g. _openmpi for Open MPI, _mpich for MPICH, ...)
    mpi_suffix = os.environ.get('MPI_SUFFIX')
    if mpi_suffix is not None:
        binaries['fdistdump'] += mpi_suffix

    # check presence of used binaries
    for key, value in binaries.items():
        if shutil.which(value) is None:
            raise OSError('executable "{}" not found.'.format(value))

    ########################################
    # process command line arguments
    # set and parse
    parser = argparse.ArgumentParser(
        description='fdistdump(1) wrapper with support for high-availability '
        'using Corosync/Pacemaker stack.')

    parser.add_argument('-m', '--mpi-args',
                        dest='user_mpi_args',
                        help='Arguments passed to the mpiexec(1) command.')
    parser.add_argument('-f', '--fdistdump-args',
                        dest='user_fdistdump_args',
                        help='Arguments passed to the fdistdump(1) command.')

    parser.add_argument('-a', '--allowed-cluster-state',
                        dest='allowed_state',
                        nargs='+',
                        choices=['online', 'offline'],
                        help='Allow the job to continue even if the '
                        'ClusterHealthMonitor enter the specified state(s). '
                        'States are relative to the reference state '
                        '(the state of the cluster during the job startup). '
                        'Reference state is always allowed.'
                        '"online" means continue even if the set of online '
                        'nodes is a superset of the refernece online nodes '
                        'set (e.g. one or more nodes went online during the '
                        'job), '
                        '"offline" means continue even if the set of '
                        'offline nodes is a superset of the reference offline '
                        'nodes set (e.g. one or more nodes went offline '
                        'during the job).')
    parser.add_argument('-i', '--health-monitoring-interval',
                        dest='health_monitoring_interval',
                        type=argument_type_timedelta,
                        default=datetime.timedelta(seconds=1),
                        help='Check changes in the health of the cluster '
                        'every HEALTH-MONITORING-INTERVAL seconds. '
                        'The defaults is 1 second.')
    parser.add_argument('-e', '--termination-timeout',
                        dest='termination_timeout',
                        type=argument_type_timedelta,
                        default=datetime.timedelta(seconds=10),
                        help='Terminate the job after specified amount of '
                        'seconds spent in one of denied states. '
                        'The default is 10 seconds.')
    parser.add_argument('-t', '--tries',
                        dest='tries',
                        type=argument_type_positive_int,
                        default=1,
                        help='After the job is terminated because the cluster '
                        'was in one of the denied states for more than '
                        'TERMINATION-TIMEOUT seconds (see the '
                        '--allowed-cluster-state and --termination-timeout '
                        'options), it can be automatically restarted with '
                        'updated refernece state according to the current '
                        'situation. This option sets number of tries before '
                        'giving up. Specify 0 for ' 'infinite retrying. '
                        'The default is 1 try.')

    parser.add_argument('-p', '--priority',
                        dest='priority',
                        choices=['low', 'normal', 'high'],
                        help='Run fdistdump with adjusted '
                        'niceness and I/O scheduling class. For more '
                        'information see nice(1) and ionice(1).')
    parser.add_argument('-v', '--verbose',
                        dest='verbose',
                        action='store_true',
                        help='Be verbose.')

    parser.add_argument('path',
                        nargs='+',
                        help='Path to your flow '
                        'file/directory relative to the "flow-primary-brick" '
                        'Pacemaker property, e.g. path will be "2016/07/" if '
                        'you want to process all flow files from July 2016.')
    args = parser.parse_args()

    # pick only relative paths from user arguments
    user_paths = []
    for path in args.path:
        if path[0] == '/':
            warnings.warn('skipping absolute path "{}"'.format(path))
        else:
            user_paths.append(path)
    if len(user_paths) == 0:
        raise ValueError('no valid path.')

    # handle priority classes
    # don't change anything if class not set or set to normal
    priority_args = []
    if args.priority == 'low':
        priority_args = ['nice', '-n', '10', 'ionice', '-c', '3']
    elif args.priority == 'high':
        priority_args = ['nice', '-n', '-10', 'ionice', '-c', '1']

    # construct fdistdump arguments
    fdistdump_args = [binaries['fdistdump']]
    if args.user_fdistdump_args:
        fdistdump_args += shlex.split(args.user_fdistdump_args)

    # construct MPI args, hosts are added later based on their current status
    mpi_args = [binaries['mpi'], '-wdir', '/tmp/', '-host']
    if args.user_mpi_args:
        mpi_args += shlex.split(args.user_mpi_args)

    # handle restarts
    DOWN_LIMIT = datetime.timedelta(seconds=5)

    # define verbose print function
    verbose_print = myformatverbose if args.verbose else lambda *a, **k: None

    # handle denied states
    if args.allowed_state:
        args.allowed_state = ClusterHealthMonitor.State(
            'online' in args.allowed_state, 'offline' in args.allowed_state)
    else:
        args.allowed_state = ClusterHealthMonitor.State()

    ########################################
    mpi_job_proc = None  # make the subprocess object global for signal handler
    tries = 0
    while True:
        tries += 1
        if args.tries == 0:
            verbose_print('try {}'.format(tries))
        elif tries <= args.tries:
            verbose_print('try {}/{}'.format(tries, args.tries))
        else:
            warnings.warn('no more tries, giving up')
            rc = None
            break

        try:
            rc = run_job()
        except ClusterHealthMonitor.ClusterHealthMonitorError:
            # cluster failure, try again with new cluster configuration
            continue
        else:
            # job success of failure, don't try again
            verbose_print('job exit code = {}'.format(rc))
            if rc < 0:
                warnings.warn('job was terminated by a signal')
            elif rc > 0:
                warnings.warn('job terminated with non-zero exit code', rc)
            break
        finally:
            if mpi_job_proc is not None and mpi_job_proc.poll() is None:
                warnings.warn('job should not be running now, but it is. '
                              'Terminating')
                terminate_job(mpi_job_proc, 5)

    ########################################
    if rc is None:
        exit(-1)
    else:
        exit(rc)
