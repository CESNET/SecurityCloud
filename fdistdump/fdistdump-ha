#!/usr/bin/env python3

# author: Jan Wrona, wrona@cesnet.cz

# Copyright (C) 2016 CESNET
#
# LICENSE TERMS
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
# 1. Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in
#    the documentation and/or other materials provided with the
#    distribution.
# 3. Neither the name of the Company nor the names of its contributors
#    may be used to endorse or promote products derived from this
#    software without specific prior written permission.
#
# ALTERNATIVELY, provided that this notice is retained in full, this
# product may be distributed under the terms of the GNU General Public
# License (GPL) version 2 or later, in which case the provisions
# of the GPL apply INSTEAD OF those given above.
#
# This software is provided ``as is'', and any express or implied
# warranties, including, but not limited to, the implied warranties of
# merchantability and fitness for a particular purpose are disclaimed.
# In no event shall the company or contributors be liable for any
# direct, indirect, incidental, special, exemplary, or consequential
# damages (including, but not limited to, procurement of substitute
# goods or services; loss of use, data, or profits; or business
# interruption) however caused and on any theory of liability, whether
# in contract, strict liability, or tort (including negligence or
# otherwise) arising in any way out of the use of this software, even
# if advised of the possibility of such damage.


import sys
import os
import argparse
import subprocess
import re
import time
import datetime
import threading
import signal
import warnings
import shutil
import shlex
import copy
import socket
from abc import ABC, abstractmethod


OUTPUT_PREFIX = os.path.basename(__file__).upper() if '__file__' in globals() \
    else 'FDISTDUMP-HA'

EXECUTABLES = {
    'node': 'crm_node',
    'mon': 'crm_mon',
    'attribute': 'crm_attribute'
}
SUBPROCESS_TIMEOUT = 5


###############################################################################
# exceptions
class Error(Exception):
    """Base class for exceptions in this module."""
    pass


###############################################################################
class Cluster:
    """
    Representation of a flow processing cluster -- nodes and attributes.
    """

    def __init__(self):
        self.__load_cluster_attributes()
        self.__load_nodes()
        self.check_subcollector_topology()
        self.update_status()

    def __repr__(self):
        return '{}(flow_original_path={}, flow_mirror_path={} ' \
            'proxy_nodes={}, subcollector_nodes={}, subcollector_graph={})' \
            .format(type(self).__name__, self.flow_original_path,
                    self.flow_mirror_path, self.proxy_nodes,
                    self.subcollector_nodes, self.subcollector_graph_str())

    def __load_nodes(self):
        self.proxy_nodes = set()
        subcollectors = set()
        for node_id, node_name in self.__get_node_list():
            role = get_cluster_attribute('role', node_name)
            if not role:
                raise Cluster.ClusterError('node "{}" has undefined the role '
                                           'attribute'.format(node_name))
            elif role == 'proxy':
                self.proxy_nodes.add(ProxyNode(node_id, node_name))
            elif role == 'subcollector':
                subcollector = SubcollectorNode(node_id, node_name)
                successor_id_str = get_cluster_attribute('successor-id',
                                                         node_name)
                if not successor_id_str:
                    raise Cluster.ClusterError('node "{}" has undefined the '
                                               'successor-id attribute'
                                               .format(node_id))
                try:
                    successor_id = int(successor_id_str)
                except:
                    raise Cluster.ClusterError('node "{}" has an invalid '
                                               'successor-id attribute value: '
                                               '{}'.format(node_id,
                                                           successor_id_str))
                subcollectors.add((subcollector, successor_id))
            else:  # role defined, but the value is not proxy nor subcollector
                raise Cluster.ClusterError('node "{}" has an invalid role '
                                           'attribute value: {}'
                                           .format(node_id, role))

        # loop through subcollectors and assign a successor and a predecessor
        # (a SubcollectorNode object) to each one of them
        self.subcollector_nodes = set()
        for subcollector, successor_id in subcollectors:
            try:
                subcollector.successor = next(sub for sub, id in subcollectors
                                              if successor_id == sub.id)
            except StopIteration:
                raise Cluster.ClusterError('node "{}" has an unmatched '
                                           'successor ID'
                                           .format(subcollector.id))
            subcollector.successor.predecessor = subcollector
            self.subcollector_nodes.add(subcollector)

        # probably a paranoid check
        if not self.proxy_nodes.isdisjoint(self.subcollector_nodes):
            raise Cluster.ClusterError('proxy nodes and subcollector nodes '
                                       'are not disjoint, this should not '
                                       'happen')

    def __load_cluster_attributes(self):
        """ Load important cluster parameters. """

        # path where original flow records are stored
        attribute_name = 'flow-original-path'
        self.flow_original_path = get_cluster_attribute(attribute_name)
        if not self.flow_original_path:
            raise Cluster.ClusterError('cluster attribute "{}" is not defined'
                                       .format(attribute_name))

        # path where mirrored flow records are stored
        attribute_name = 'flow-mirror-path'
        self.flow_mirror_path = get_cluster_attribute(attribute_name)
        if not self.flow_mirror_path:
            raise Cluster.ClusterError('cluster attribute "{}" is not defined'
                                       .format(attribute_name))

    def __get_node_list(self):
        """
        crm_node: tool for displaying low-level node information
            --list: print all known members (past and present) of this cluster

        output:
            <ID> <NAME> member
            ...

        return:
            list of tuples (integer node ID, string name (usually hostname))
        """

        args = [EXECUTABLES['node'], '--list']
        proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                universal_newlines=True)
        stdout_str, stderr_str = proc.communicate()
        ret_code = proc.wait(SUBPROCESS_TIMEOUT)

        if ret_code != 0 or stderr_str:
            err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: ' +
                       str(ret_code) + '\nstderr:' + stderr_str)
            raise subprocess.SubprocessError(err_str)

        node_list = []
        for match in re.findall(r'(\d+)\s+(\S+)', stdout_str):
            node_list.append((int(match[0]), match[1]))

        return node_list

    def check_subcollector_topology(self):
        """
        Check validity of the subcollector topology: a graph consisting of a
        single component forming a directed cycle.
        """

        if len(self.subcollector_nodes) == 0:
            raise Cluster.ClusterError('no subcollector node found')

        subcollectors = copy.copy(self.subcollector_nodes)
        sub = sub_first = subcollectors.pop()  # return and remove
        subcollectors.add(sub)  # put it back in

        if sub_first == sub_first.successor:
            raise Cluster.ClusterError('subcollector self-loop does not make '
                                       'sense')

        while sub.successor in subcollectors:
            subcollectors.remove(sub.successor)  # remove the successor
            sub = sub.successor  # move to the successor

        # check if the last is also the first (cycle)
        if sub != sub_first:
            raise Cluster.ClusterError('subcollector graph does not form a '
                                       'cycle')
        # check if all we looped through all subcollectors (single component)
        if subcollectors:
            raise Cluster.ClusterError('subcollector graph contains more than '
                                       'one component')

    def subcollector_graph_str(self):
        """ Print the subcollector topology graph. """

        self.check_subcollector_topology()

        first = min(self.subcollector_nodes, key=lambda sub: sub.id)
        graph_str = str(first.id)

        sub = first.successor
        while sub != first:
            graph_str += ' -> ' + str(sub.id)
            sub = sub.successor
        return graph_str

    def update_status(self):
        """
        Without --hide-headers crm_mon also shows Last updated, but it is not
        needed right now.

        crm_mon: provides a summary of cluster's current state (online,
                 offline, standby, ...)
            --group-by-node: group resources by node
            --hide-headers:  hide all headers (good for parsing)
            --show-detail:   show more details (node IDs, individual clone
                             instances)
            --one-shot:      display the cluster status once on the console
                             and exit (no ncurses)

        output:
            Node <NAME> (<ID>): <STATUS>
            ...

        return:
            list of tuples (integer ID, string status)
        """

        args = [EXECUTABLES['mon'], '--group-by-node', '--hide-headers',
                '--show-detail', '--one-shot']
        proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                universal_newlines=True)
        stdout_str, stderr_str = proc.communicate()
        ret_code = proc.wait(SUBPROCESS_TIMEOUT)

        if ret_code != 0 or stderr_str:
            err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: '
                       + str(ret_code) + '\nstderr:' + stderr_str)
            raise subprocess.SubprocessError(err_str)

        matches = re.findall(r'Node\s+(\S+)\s+\((\d+)\):\s+(\w+)', stdout_str)
        if not matches:
            raise Cluster.ClusterError('error while parsing the output of the '
                                       'crm_mon command')
        status_dict = dict()
        for match in matches:
            # 0 = node name, 1 = ID string, 2 = status string
            node_id = int(match[1])
            if node_id in status_dict:
                raise Cluster.ClusterError('duplicate node IDs in the output '
                                           'of the crm_mon command')
            status_dict[node_id] = (match[0], match[2])

        for node in self.proxy_nodes | self.subcollector_nodes:
            name, node.status = status_dict[node.id]
            if node.name != name:
                raise Cluster.ClusterError('node name mismatch in the output '
                                           'of the crm_mon command')

    def get_online_nodes(self):
        return {i for i in self.proxy_nodes | self.subcollector_nodes if
                i.is_online()}

    def get_online_proxy_nodes(self):
        return {i for i in self.proxy_nodes if i.is_online()}

    def get_online_subcollector_nodes(self):
        return {i for i in self.subcollector_nodes if i.is_online()}

    class ClusterError(Error):
        """General Cluster exception.

        Attributes:
            message -- explanation of the error
        """

        def __init__(self, message):
            self.message = message


class Node(ABC):
    """Abstract representation of a node in a flow processing cluster."""

    @abstractmethod
    def __init__(self, id, name):
        self.id = int(id)
        self.name = name
        self.status = None

    def __repr__(self):
        return '{}(id={}, name={}, status={})'.format(type(self).__name__,
                                                      self.id, self.name,
                                                      self.status)

    def is_online(self):
        return self.status == 'online'


class ProxyNode(Node):
    """
    Object representing a proxy node in a flow processing cluster.
    """

    def __init__(self, id, name):
        super().__init__(id, name)


class SubcollectorNode(Node):
    """
    Object representing a subcollector node in a flow processing cluster.
    """

    def __init__(self, id, name):
        super().__init__(id, name)
        self.successor = None
        self.predecessor = None

    def __repr__(self):
        successor_id = self.successor.id if self.successor else None
        predecessor_id = self.predecessor.id if self.predecessor else None
        return '{}, successor.id={}, predecessor.id={})' \
            .format(super().__repr__()[:-1], successor_id, predecessor_id)


###############################################################################
def verbose_print_formatter(*args, **kwargs):
    print(OUTPUT_PREFIX, 'VERBOSE:', *args, **kwargs)


def verbose_print_formatter_void(*args, **kwargs):
    pass


def myformatwarning(message, category, filename, lineno, line=None):
    return '{} WARNING: {}\n'.format(OUTPUT_PREFIX, str(message))


def check_executables():
    """ Check presence of the defined executables. """
    for key, value in EXECUTABLES.items():
        if shutil.which(value) is None:
            raise OSError('executable "{}" not found.'.format(value))


def get_cluster_attribute(attribute_name, node_name=None):
    """
    crm_attribute: manage node's attributes and cluster options
        --query:     query the current value of the attribute/option
        --name=NAME: name of the attribute/option to operate on
        --node=NODE: get an attribute for the named node (instead of a
                     cluster option)

    output:
        scope=nodes  name=<ATTRIBUTE> value=<NODE>

    return:
        attribute value as string or None if no such attribute is set
    """

    args = [EXECUTABLES['attribute'], '--query',
            '--name={}'.format(attribute_name)]
    if node_name:
        args.append('--node={}'.format(node_name))
    proc = subprocess.Popen(args, stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE, universal_newlines=True)
    stdout_str, stderr_str = proc.communicate()
    ret_code = proc.wait(SUBPROCESS_TIMEOUT)

    if ret_code != 0 or stderr_str:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nreturn code: '
                   + str(ret_code) + '\nstderr:' + stderr_str)
        raise subprocess.SubprocessError(err_str)

    match = re.fullmatch(r'scope={}\s+name={}\s+value=(\S+)\n'.format('nodes'
                         if node_name else 'crm_config', attribute_name),
                         stdout_str)
    if not match:
        err_str = ('\ncommand: ' + ' '.join(args) + '\nbad output:\n'
                   + stdout_str)
        raise subprocess.SubprocessError(err_str)

    return match.group(1)


def terminate_job(job_popen, timeout):
    verbose_print('termination: sending SIGTERM')
    job_popen.terminate()
    try:
        job_popen.wait(timeout)
    except subprocess.TimeoutExpired:
        verbose_print('termination: SIGTERM did not work. Sending SIGKILL')
        job_popen.kill()
        try:
            job_popen.wait(timeout)
        except subprocess.TimeoutExpired:
            verbose_print('termination: SIGKILL did not work, giving up')
        else:
            verbose_print('termination: SIGKILL has terminated the job')
    else:
        verbose_print('termination: SIGTERM has terminated the job')


def signal_handler(signal, frame):
    """
    When signal is sent to the process group (which is the same for the
    fdistdump-ha and fdistdump), MPI process manager will terminate the job
    in a usual way, but it can take a while (therefore the first wait).

    But when signal is sent only to the fdistdump-ha process or MPI process
    manager for some reason couldn't terminate the job by itself, its our
    responsibility to terminate the job -- first by SIGTERM, then by SIGKILL.
    """

    warnings.warn('termination: received signal {}, terminating job and '
                  'exitting as soon as possible'.format(signal))

    if ('mpi_job_proc' not in globals()
            or mpi_job_proc is None
            or mpi_job_proc.poll() is not None):
        verbose_print('termination: job is not running, exiting immediately')
    else:
        verbose_print('termination: job is running, waiting {} seconds for '
                      'the job to self-terminate'.format(SUBPROCESS_TIMEOUT))
        try:
            mpi_job_proc.wait(SUBPROCESS_TIMEOUT)
        except subprocess.TimeoutExpired:
            verbose_print('termination: the job did not self-terminate')
            terminate_job(mpi_job_proc, SUBPROCESS_TIMEOUT)
        else:
            verbose_print('termination: the job has self-terminated')

    sys.exit()  # only raises the SystemExit exception


class ClusterHealthMonitor:
    """Cluster health monitoring class."""

    def __init__(self, online_ref_set, termination_timeout, allowed_state):
        self.state = self.State()
        self.online_ref_set = online_ref_set
        self.online_set = online_ref_set
        self.termination_timeout = termination_timeout
        self.termination_timer = threading.Timer(self.termination_timeout,
                                                 self._terminate)
        self.allowed_state = allowed_state
        self.terminated = False

    def __del(self):
        self.termination_timer.cancel()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.termination_timer.cancel()

    def _terminate(self):
        warnings.warn('{}: time spent in the denied state exceeded its limit, '
                      'terminating the job'.format(type(self).__name__))
        terminate_job(mpi_job_proc, SUBPROCESS_TIMEOUT)
        self.terminated = True

    def update(self, online_set_new):
            # compare the current online node set to the previous one
            if online_set_new != self.online_set:
                # one or more nodes went online, offline, or combination
                online_superset = online_set_new - self.online_set
                offline_superset = self.online_set - online_set_new

                for node in online_superset:
                    warnings.warn('{}: node {} has upgraded to the online '
                                  'status'.format(type(self).__name__,
                                                  node.name))
                for node in offline_superset:
                    warnings.warn('{}: node {} has degraded from the online '
                                  'status'.format(type(self).__name__,
                                                  node.name))

            # compare the current online node set to the reference one
            online_superset = online_set_new - self.online_ref_set
            offline_superset = self.online_ref_set - online_set_new
            state_new = self.State(bool(online_superset),
                                   bool(offline_superset))

            # check if the state has changed
            if self.state != state_new:
                state_status = 'denied' if \
                    self.state.is_denied(self.allowed_state) else 'allowed'
                state_new_status = 'denied' if \
                    state_new.is_denied(self.allowed_state) else 'allowed'

                warnings.warn('{}: transition from the {} state <{}> to the '
                              '{} state <{}>'.format(type(self).__name__,
                                                     state_status, self.state,
                                                     state_new_status,
                                                     state_new))
                if state_new.is_denied(self.allowed_state):
                    # transition to the denied state, start the timer
                    if not self.termination_timer.is_alive():
                        self.termination_timer.start()
                else:
                    # transition to the allowed state, cancel the timer
                    self.termination_timer.cancel()
                    self.termination_timer = threading.Timer(
                        self.termination_timeout, self._terminate)

            self.state.update(state_new)
            self.online_set = online_set_new

    class State:
        """Class representing state of the ClusterHealthMonitor."""

        def __init__(self, online=False, offline=False):
            self._online = bool(online)
            self._offline = bool(offline)

        def __eq__(self, other):
            return (self._online == other._online
                    and self._offline == other._offline)

        def __or__(self, other):
            return (self._online == other._online
                    or self._offline == other._offline)

        def __str__(self):
            if self._online and self._offline:
                return 'superset of both online and offline nodes'
            elif self._online:
                return 'superset of online nodes'
            elif self._offline:
                return 'superset of offline nodes'
            else:
                return 'reference'

        def update(self, other):
            self._online = other._online
            self._offline = other._offline

        def is_denied(self, other):
            return (self._online and not other._online
                    or self._offline and not other._offline)

    class ClusterHealthMonitorError(Error):
        """General ClusterHealthMonitor exception.

        Attributes:
            message -- explanation of the error
        """

        def __init__(self, message):
            self.message = message


def run_job(ha_args, mpiexec_exec, mpiexec_args, priority_args, fdistdump_exec,
            fdistdump_args, user_paths):
    cluster = Cluster()

    online_proxy_nodes = cluster.get_online_proxy_nodes()
    online_subcollector_nodes = cluster.get_online_subcollector_nodes()

    # at least one online subcollector node is mandatory
    if not online_subcollector_nodes:
        raise Exception('no online subcollector node')

    # choose a master for the query -- prefer local node over remote nodes and
    # dedicated proxy nodes over subcollector nodes
    fqdn = socket.getfqdn()
    local_names = [fqdn, fqdn.partition('.')[0], socket.gethostname()]
    for node in online_proxy_nodes | online_subcollector_nodes:
        # select localhost, whether it is a proxy node or a subcollector node
        if node.name in local_names:
            master_node = node
            break
    else:
        # select an arbitrary online proxy node or subcollector node
        master_node = next(iter(online_proxy_nodes)) if online_proxy_nodes \
            else next(iter(cluster.subcollector_nodes))
        warnings.warn('using remote node "{}" as a master node'
                      .format(master_node.name))
    mpiexec_hostnames = [master_node.name]

    # fdistdump paths are contatenation of flow_original_path and/or
    # flow_mirror_path with user specified user_paths
    original_paths = [os.path.join(cluster.flow_original_path, user_path) for
                      user_path in user_paths]
    mirror_paths = [os.path.join(cluster.flow_mirror_path, user_path) for
                    user_path in user_paths]

    if cluster.subcollector_nodes == online_subcollector_nodes:
        # all subcollectors are online, use the MPI Single Process Multiple
        # Data (SPMD) model
        mpiexec_hostnames += [sub.name for sub in online_subcollector_nodes]
        local_args = priority_args + [fdistdump_exec] + fdistdump_args \
            + original_paths
    else:
        # one or more subcollecors are offline, use the the MPI colon notation
        # for Multiple Program Multiple Data (MPMD) model
        # mpiexec -host node1,node2 \                        # global arguments
        #     -n 1 fdistdump flow_original_path : \          # local arguments
        #     -n 1 fdistdump flow_original_path flow_mirror_path
        LOCAL_ARGS_CONST = ['-n', '1'] + priority_args + [fdistdump_exec] \
            + fdistdump_args + original_paths
        local_args = LOCAL_ARGS_CONST[:-1] + ['/fake/unused/path']

        sub = first = min(cluster.subcollector_nodes, key=lambda sub: sub.id)
        while True:
            if sub.is_online():
                mpiexec_hostnames.append(sub.name)

                local_args.append(':')
                local_args += LOCAL_ARGS_CONST

                if not sub.predecessor.is_online():
                    warnings.warn('node "{}" is not online, using the mirror '
                                  'on node "{}" instead'
                                  .format(sub.predecessor.name, sub.name))
                    local_args += mirror_paths

            sub = sub.successor  # move to the successor
            if sub == first:
                break

    # construct global arguments for both healthy and sick cluster states
    subprocess_args = [mpiexec_exec] + mpiexec_args \
        + ['-host', ','.join(mpiexec_hostnames)]
    # append local arguments specific for healthy/sick cluster states
    subprocess_args += local_args

    # launch the job
    verbose_print('launching:', ' '.join([shlex.quote(arg) for arg in
                                          subprocess_args]))
    # Make sure stdin is not associated with the terminal. MPI process manager
    # reads from stdin, which causes many problems (SIGTTIN used for background
    # jobs, bash while read loop, etc.) and fdistdump does not need stdin.
    global mpi_job_proc
    mpi_job_proc = subprocess.Popen(subprocess_args, stdin=subprocess.DEVNULL)

    # start the monitoring loop
    interval = ha_args.health_monitoring_interval.total_seconds()
    with ClusterHealthMonitor(cluster.get_online_nodes(), interval,
                              ha_args.allowed_state) as chm:
        while mpi_job_proc.poll() is None:
            cluster.update_status()
            chm.update(cluster.get_online_nodes())
            time.sleep(ha_args.health_monitoring_interval.total_seconds())
        else:
            if chm.terminated:
                raise ClusterHealthMonitor.ClusterHealthMonitorError(
                    'job terminated by the ClusterHealthMonitor')

    # process has finished
    return mpi_job_proc.returncode


def argument_type_positive_int(value_str):
    try:
        value_int = int(value_str)
    except:
        exc_msg = sys.exc_info()[1]
        raise argparse.ArgumentTypeError(exc_msg)

    if value_int < 0:
        raise argparse.ArgumentTypeError('negative value is not allowed')
    else:
        return value_int


def argument_type_timedelta(seconds_str):
    seconds_int = argument_type_positive_int(seconds_str)
    try:
        delta = datetime.timedelta(seconds=seconds_int)
    except:
        exc_msg = sys.exc_info()[1]
        raise argparse.ArgumentTypeError(exc_msg)

    return delta


###############################################################################
def main():
    ########################################
    # attach handler to SIGINT and SIGTERM for gracefull job termination
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # check if all needed executables are installed
    check_executables()

    # set my format for warning print function
    warnings.formatwarning = myformatwarning

    # split arguments into fdistdump-ha, mpiexec, and fdistdump arguments
    hyphens_idx = next((idx for idx, val in enumerate(sys.argv[1:], 1) if
                       val == '--'), sys.maxsize)
    mpiexec_argv_idx = next((idx for idx, val in enumerate(sys.argv[1:], 1) if
                            'mpiexec' in val), sys.maxsize)
    fdistdump_argv_idx = next((idx for idx, val in enumerate(sys.argv[1:], 1)
                              if 'fdistdump' in val), sys.maxsize)
    if hyphens_idx < mpiexec_argv_idx and hyphens_idx < fdistdump_argv_idx:
        # abbreviated form with FDISTDUMP_ARGS
        ha_argv = sys.argv[:hyphens_idx]
        fdistdump_argv = [None] + sys.argv[hyphens_idx + 1:]
        mpiexec_argv = [None]
    elif mpiexec_argv_idx < fdistdump_argv_idx:
        # full form, MPIEXEC first
        ha_argv = sys.argv[:mpiexec_argv_idx]
        mpiexec_argv = sys.argv[mpiexec_argv_idx:fdistdump_argv_idx]
        fdistdump_argv = (sys.argv[fdistdump_argv_idx:] if fdistdump_argv_idx <
                          sys.maxsize else [None])
    elif fdistdump_argv_idx < mpiexec_argv_idx:
        # full form, FDISTDUMP first
        ha_argv = sys.argv[:fdistdump_argv_idx]
        mpiexec_argv = (sys.argv[mpiexec_argv_idx:] if mpiexec_argv_idx <
                        sys.maxsize else [None])
        fdistdump_argv = sys.argv[fdistdump_argv_idx:mpiexec_argv_idx]
    else:
        # abbreviated form without FDISTDUMP_ARGS
        ha_argv = copy.copy(sys.argv)
        mpiexec_argv = [None]
        fdistdump_argv = [None]

    ha_exec = ha_argv.pop(0)  # extract fdistdump-ha executable

    ########################################
    # parse fdistdump-ha's command line arguments
    parser = argparse.ArgumentParser(
        description='This script is a wrapper for fdistdump(1) with support '
        'for high-availability (which lacks in most MPI implementations) '
        'using a Corosync/Pacemaker stack. '

        'MPIEXEC is either a name of or an absolute path to the mpiexec(1) '
        'command (default: mpiexec). In the first case, the command is '
        'located using the PATH environment variable. It is identified as the '
        'first argument containing substring "mpiexec". '
        'MPIEXEC_ARGS are arguments passed to the mpiexec(1) command '
        '(default: None). Do not use the "-host" option, because its argument '
        'is constructed automatically in runtime (based on current nodes '
        'health). '

        'FDISTDUMP is either a name of or an absolute path to the '
        'fdistdump(1) command. In the first case, the command is located '
        'using the PATH environment variable. On some Fedora-based '
        'distributions MPI executables must be suffixed with a MPI_SUFFIX '
        'environment variable (e.g., _openmpi for Open MPI or _mpich for '
        'MPICH). The default value is thus "fdistdump$MPI_SUFFIX" if '
        'MPI_SUFFIX is defined, "fdistdump" otherwise. It is identified as '
        'the first argument containing substring "fdistdump". Instead of '
        'specifying paths here, use the positional argument in the {}_ARGS. '
        'FDISTDUMP_ARGS are arguments passed to the fdistdump(1) command '
        '(default: None).'
        .format(os.path.basename(sys.argv[0]).upper()),

        usage='\t{0} [{1}_ARGS] [-- FDISTDUMP_ARGS]\n'
        '\t{0} [{1}_ARGS] [MPIEXEC [MPIEXEC_ARGS]] [FDISTDUMP '
        '[FDISTDUMP_ARGS]]'.format(os.path.basename(sys.argv[0]),
                                   os.path.basename(sys.argv[0]).upper()),
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('-a', '--allowed-cluster-state',
                        dest='allowed_state',
                        nargs='+',
                        choices=['online', 'offline'],
                        default=None,
                        help='Allow the job to continue even if the '
                        'ClusterHealthMonitor enter the specified state(s). '
                        'States are relative to the reference state '
                        '(the state of the cluster during the job startup). '
                        'Reference state is always allowed.'
                        '"online" means continue even if the set of online '
                        'nodes is a superset of the refernece online nodes '
                        'set (e.g. one or more nodes went online during the '
                        'job), '
                        '"offline" means continue even if the set of '
                        'offline nodes is a superset of the reference offline '
                        'nodes set (e.g. one or more nodes went offline '
                        'during the job).')
    parser.add_argument('-i', '--health-monitoring-interval',
                        dest='health_monitoring_interval',
                        type=argument_type_timedelta,
                        default=datetime.timedelta(seconds=1),
                        help='Check changes in the health of the cluster '
                        'every HEALTH-MONITORING-INTERVAL seconds.')
    parser.add_argument('-e', '--termination-timeout',
                        dest='termination_timeout',
                        type=argument_type_timedelta,
                        default=datetime.timedelta(seconds=10),
                        help='Terminate the job after specified amount of '
                        'seconds spent in one of denied states.')
    parser.add_argument('-t', '--tries',
                        dest='tries',
                        type=argument_type_positive_int,
                        default=1,
                        help='After the job is terminated because the cluster '
                        'was in one of the denied states for more than '
                        'TERMINATION-TIMEOUT seconds (see the '
                        '--allowed-cluster-state and --termination-timeout '
                        'options), it can be automatically restarted with '
                        'updated refernece state according to the current '
                        'situation. This option sets number of tries before '
                        'giving up. Specify 0 for ' 'infinite retrying.')

    parser.add_argument('-p', '--priority',
                        dest='priority',
                        choices=['low', 'normal', 'high'],
                        default='normal',
                        help='Run fdistdump with adjusted '
                        'niceness and I/O scheduling class. For more '
                        'information see nice(1) and ionice(1).')
    parser.add_argument('-v', '--verbose',
                        dest='verbose',
                        action='store_true',
                        default=False,
                        help='Be verbose.')

    parser.add_argument('path',
                        nargs='+',
                        help='Path to your flow file/directory relative to '
                        'the "flow-primary-brick" Pacemaker property. If you '
                        'are not using profiles, it can be just "2016/07/" if '
                        'you want to process all flow files from July 2016. '
                        'If you are using profiles, your path should start '
                        'with a name of the required profile, e.g., '
                        '"live/channels/smtp/2016/07/" if you want to '
                        'process all flow files belonging to profile "live", '
                        'channel "smtp" from July 2016.')
    ha_args = parser.parse_args(ha_argv)

    ########################################
    # validate and handle command line arguments
    # define verbose print function
    global verbose_print
    if ha_args.verbose:
        verbose_print = verbose_print_formatter
    else:
        verbose_print = verbose_print_formatter_void

    # construct and check mpiexec args, hosts are added later
    mpiexec_exec = mpiexec_argv.pop(0)
    if mpiexec_exec is None:
        mpiexec_exec = 'mpiexec'
        verbose_print('arguments: MPIEXEC is not defined, using default '
                      'value "{}"'.format(mpiexec_exec))
    elif shutil.which(mpiexec_exec) is None:
        raise ValueError('"{}" is not a valid mpiexec command (PATH '
                         'lookup failed)'.format(mpiexec_exec))
    if '-host' in mpiexec_argv:
        raise ValueError('arguments: mpiexec argument "-host" is forbidden')
    if '-wdir' not in mpiexec_argv:
        mpiexec_argv += ['-wdir', '/tmp/']

    # construct and check fdistdump args
    fdistdump_exec = fdistdump_argv.pop(0)
    if fdistdump_exec is None:
        mpi_suffix = os.environ.get('MPI_SUFFIX')
        if mpi_suffix is None:
            fdistdump_exec = 'fdistdump'
        else:
            fdistdump_exec = 'fdistdump' + mpi_suffix
        verbose_print('arguments: FDISTDUMP is not defined, using default '
                      'value "{}"'.format(fdistdump_exec))
    # test fdistdump command existence
    if shutil.which(fdistdump_exec) is None:
        # only warning, command may be valid on the remote nodes
        warnings.warn('arguments: "{}" is not a valid fdistdump command on '
                      'this node (PATH lookup failed)'
                      .format(fdistdump_exec))

    verbose_print('arguments:', ha_exec, ha_argv)
    verbose_print('arguments:', mpiexec_exec, mpiexec_argv)
    verbose_print('arguments:', fdistdump_exec, fdistdump_argv)

    # pick only relative paths from user arguments
    user_paths = []
    for path in ha_args.path:
        if path[0] == '/':
            warnings.warn('arguments: skipping absolute path "{}"'
                          .format(path))
        else:
            user_paths.append(path)
    if len(user_paths) == 0:
        raise ValueError('arguments: no valid path found')

    # handle priority classes
    if ha_args.priority == 'low':
        priority_args = ['nice', '-n', '10', 'ionice', '-c', '3']
    elif ha_args.priority == 'normal':
        priority_args = []  # don't change anything if class is set to normal
    elif ha_args.priority == 'high':
        priority_args = ['nice', '-n', '-10', 'ionice', '-c', '1']
    else:
        raise ValueError('arguments: invalid priority class "{}"'
                         .format(ha_args.priority))

    # handle denied states
    if ha_args.allowed_state:
        ha_args.allowed_state = ClusterHealthMonitor.State(
            'online' in ha_args.allowed_state,
            'offline' in ha_args.allowed_state)
    else:
        ha_args.allowed_state = ClusterHealthMonitor.State()

    ########################################
    # make the subprocess object global for the signal handler
    mpi_job_proc = None
    tries = 0
    while True:
        tries += 1
        if ha_args.tries == 0:
            verbose_print('run: try {}'.format(tries))
        elif tries <= ha_args.tries:
            verbose_print('run: try {}/{}'.format(tries, ha_args.tries))
        else:
            warnings.warn('run: no more tries, giving up')
            rc = None
            break

        try:
            rc = run_job(ha_args, mpiexec_exec, mpiexec_argv, priority_args,
                         fdistdump_exec, fdistdump_argv, user_paths)
        except ClusterHealthMonitor.ClusterHealthMonitorError:
            # cluster failure, try again with new cluster configuration
            continue
        else:
            # job success of failure, don't try again
            verbose_print('run: job exit code = {}'.format(rc))
            if rc < 0:
                warnings.warn('run: job was terminated by a signal')
            elif rc > 0:
                warnings.warn('run: job terminated with non-zero exit code {}'
                              .format(rc))
            break
        finally:
            if mpi_job_proc is not None and mpi_job_proc.poll() is None:
                warnings.warn('run: job should not be running now, but it is. '
                              'Terminating')
                terminate_job(mpi_job_proc, SUBPROCESS_TIMEOUT)

    ########################################
    if rc is None:
        exit(-1)
    else:
        exit(rc)


###############################################################################
if __name__ == "__main__":
    main()
